{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home_lab_local/s1810410/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home_lab_local/s1810410/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home_lab_local/s1810410/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home_lab_local/s1810410/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home_lab_local/s1810410/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from gensim.models import KeyedVectors\n",
    "from urllib import request\n",
    "import string\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem.porter import *\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer  \n",
    "from nltk import pos_tag\n",
    "porter_stemmer = PorterStemmer() \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "punc = '[,.!\\')(-;?:''\"``\"\"]'\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_dict(file):\n",
    "    f = open(file, 'r')\n",
    "    js =f.read()\n",
    "    DIC = json.loads(js)\n",
    "    f.close()\n",
    "    return DIC\n",
    "\n",
    "# 获取字典信息\n",
    "f1 = open('corpus/GlossDict.txt', 'r') \n",
    "f2 = open('corpus/GlossDictExpand.txt', 'r')\n",
    "dic = eval(f1.read())   # str into dict\n",
    "dic2 = eval(f2.read())\n",
    "glossDict = dic\n",
    "ExpandglossDict = dic2\n",
    "f1.close()\n",
    "f2.close()\n",
    "\n",
    "# 获取文章信息和文章ID\n",
    "f3 = open('corpus/ID.txt', 'r')\n",
    "f4 = open('corpus/INSTANCE.txt', 'r')\n",
    "id_list = [] # 3944\n",
    "article = [] # 3944\n",
    "for line in f3.readlines():\n",
    "    line = line.strip('\\n')\n",
    "    line = line.strip('\"')\n",
    "    id_list.append(line)\n",
    "\n",
    "for art in f4.readlines():\n",
    "    art = art.strip('\\n')\n",
    "    art = art.strip('[,],\\\\n,\",')\n",
    "    article.append(art)\n",
    "f3.close()\n",
    "f4.close()\n",
    "f6 = open('corpus/GlossDictWITHpunc.txt', 'r') \n",
    "dic6 = eval(f6.read())   # str into dict\n",
    "glossWithPunc = list(dic6)\n",
    "f6.close()\n",
    "EPGwithSTOP = open('corpus/EPGwithSTOP.txt', 'r') \n",
    "dicEPG = eval(EPGwithSTOP.read())  \n",
    "EPGwith = list(dicEPG)                                 # wrod 保留stopwords； 额外信息没有stopwords\n",
    "EPGwithSTOP.close()\n",
    "sentence_file = open(r\"corpus/corp.txt\", \"r\")\n",
    "out = sentence_file.read()\n",
    "sentence_corpus = json.loads(out)\n",
    "sentence_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "glossAND_stop = load_dict('./corpus/glossAND_stop_dict.txt') # 包含stopwords的词典信息\n",
    "glossDict = dic                                              # 不包含stopwrods的词典信息\n",
    "hypANDstop = load_dict('./corpus/hypANDstop_dict.txt')    # 加入额外信息的包含stopwords的词典信息\n",
    "hypAND = load_dict('./corpus/hypAND_dict.txt')            # 加入额外信息的不包含stopwords的词典信息\n",
    "\n",
    "EPGwith = list(dicEPG)           # 加入的额外信息不包含stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''new code start '''\n",
    "def find_depend_collo(target,test):\n",
    "    # input 一个句子， 一个目标单词\n",
    "    # 返回 这个句子中 和目标单词有依存关系的 词组\n",
    "    res = dependency_parser.raw_parse(test)\n",
    "    tree = list(res.__next__().triples())\n",
    "    collocations = []\n",
    "    for child in tree:\n",
    "        temp = []\n",
    "        for i in child:\n",
    "            if len(i)>2:\n",
    "                temp.append(i)\n",
    "            else:\n",
    "                temp.append(i[0])\n",
    "        if target in temp:\n",
    "            collocations.append(temp[0]+ ' '+ temp[2]+' '+temp[1])\n",
    "    return collocations\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "noun_key_value = re.compile(r'[a-z]+[%][\\d]+[:][\\d]+[:][\\d]+[:]+[a-z]*[:]*[\\d\\d]*')\n",
    "def RE(S):\n",
    "    tag = re.findall('\\d\\d+', S)[0]  # extraction number\n",
    "    collo = re.sub(' [0-9]+','',S)  #  sub number\n",
    "    return tag, collo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip =['he', 'her', 'his', 'a', 'an', 'the', 'that', 'them', 'which', 'her', 'i', 'be', 'my', 'their', 'what', 'where']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag answer by dependecy collocation feature\n",
    "def GetQualtity(jsonfile, LS, i):\n",
    "    LS = LS[i]\n",
    "    \n",
    "    # get quality collo by filt-score\n",
    "    with open(jsonfile, 'r') as f:\n",
    "        rawdata = f.readlines()\n",
    "        rawdata = [i.strip('\\n') for i in rawdata]\n",
    "        rawdata = [i.split(' ') for i in rawdata]\n",
    "        data = []\n",
    "        for i in rawdata:\n",
    "            collo = i[0] + ' ' + i[1] + ' ' + i[3]\n",
    "            sense = i[4]\n",
    "            data.append((collo, sense))\n",
    "\n",
    "    a = {}\n",
    "    b = {}\n",
    "    for item in data:\n",
    "        co = item[0]\n",
    "        se = item[1]\n",
    "        if co not in a:\n",
    "            a[co] = 1\n",
    "        else:\n",
    "            a[co] += 1\n",
    "\n",
    "        co_se = co + ' ' + se\n",
    "        if co_se not in b:\n",
    "            b[co_se] = 1\n",
    "        else:\n",
    "            b[co_se] += 1\n",
    "\n",
    "    reliable = {}\n",
    "    for c,n in b.items():\n",
    "        k = RE(c)[1]  # RE -> V  ; RE2 -> N\n",
    "        #k = k.strip()  # N -> remove space\n",
    "        if k in skip:\n",
    "            continue\n",
    "        else:\n",
    "            if k in a:\n",
    "                appear_n = n\n",
    "                score = n/a[k]\n",
    "                if appear_n > 5 and score >= 0.7:    # (appear_n 5, score 0.7)\n",
    "                    sense = c.split(' ')[-1]\n",
    "                    reliable[k] = sense\n",
    "    \n",
    "                    \n",
    "    # tag answer\n",
    "    res = Find_collo(LS[2], LS[0], LS[1], 9)  # play test data \n",
    "    play_ans = []\n",
    "    \n",
    "    for context in res[1]:\n",
    "        flag = 0\n",
    "        context_co = find_depend_collo(LS[2], ' '.join(context))\n",
    "        # 存储临时tag， 因为一个句子可能有多个tag，选其中出现次数最多的作为true tag\n",
    "        temp_tag = {}\n",
    "        for n,item in enumerate(context_co):\n",
    "            if item in reliable:  \n",
    "                tag = reliable[item]  \n",
    "                if tag not in temp_tag:\n",
    "                    temp_tag[tag] = 1\n",
    "                else:\n",
    "                    temp_tag[tag] += 1              \n",
    "        # sort reverse select most frequency tag\n",
    "        if len(temp_tag)>0:\n",
    "            tag = sorted(temp_tag.items(), key= lambda x:x[1], reverse=True)[0]\n",
    "            if temp_tag[tag[0]] >= 1:\n",
    "                play_ans.append(tag)\n",
    "                flag = 1\n",
    "        # compute sim by base method if no matching\n",
    "        if flag == 0:\n",
    "            tag =  ' ' #Tag_origi(context, glossDict[LS[3]:LS[4]], LS[2], 1)\n",
    "            play_ans.append(tag)\n",
    "            \n",
    "    # write answer into 'test_new'     \n",
    "    test = play_ans \n",
    "    test_new= []   \n",
    "    for i in test:\n",
    "        if type(i) == tuple:\n",
    "            test_new.append(i[0])\n",
    "        else:\n",
    "            test_new.append(i)\n",
    "            \n",
    "    return test_new, reliable, b, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer 词性还原\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    res = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word, pos in pos_tag(word_tokenize(sentence)):\n",
    "        wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "        res.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "    return res\n",
    "\n",
    "article = list(map(lambda x: ' '.join(lemmatize_sentence(x)), article))\n",
    "\n",
    "# clean data with oov, punc, lower characters\n",
    "def clean(word_list):\n",
    "    new_sent = word_list\n",
    "    new_sent = [word for word in new_sent if word in word_vectors.wv.vocab]\n",
    "    new_sent = [word for word in new_sent if word not in punc]\n",
    "    new_sent = [w.lower() for w in new_sent]\n",
    "   #new_sent = [word for word in new_sent if word not in stop_words]\n",
    "    return new_sent\n",
    "\n",
    "# delete short sentences in corpus, lemmatize\n",
    "def txt2wordlist(text):\n",
    "    afterClean = []\n",
    "    text = sent_tokenize(text)\n",
    "    for i in text:\n",
    "        i = lemmatize_sentence(i)\n",
    "        i = clean(i)\n",
    "        if len(i) > 5:\n",
    "            afterClean.append(i)\n",
    "    return afterClean\n",
    "\n",
    "# find collocations for a target word in test data and windowsize words\n",
    "def Find_collo(keyword,s,e,n):\n",
    "    collocation = []\n",
    "    KeyWord_art = []\n",
    "    target = article[s:e]\n",
    "    \n",
    "    # new code\n",
    "    for art in target:\n",
    "        art = art.lower()\n",
    "        seg_sent = sent_tokenize(art)\n",
    "        for s in seg_sent:\n",
    "            lem_s = lemmatize_sentence(s)\n",
    "            lem_s = [w for w in lem_s if w not in punc]\n",
    "            lem_s = clean(lem_s)\n",
    "            \n",
    "            ans = [j for j,x in enumerate(lem_s) if x.find(keyword)!=-1]\n",
    "            if len(ans)>0:\n",
    "                ind = ans[0]\n",
    "                if len(lem_s) > 10 and ind >= 5: \n",
    "                    ret = lem_s[ind-4:ind+4]\n",
    "                elif len(lem_s) > 10 and ind < 5:\n",
    "                    ret = lem_s[:ind+5]\n",
    "                else:\n",
    "                    ret = lem_s\n",
    "                KeyWord_art.append(ret)\n",
    "                break\n",
    "    # new code over\n",
    "    '''\n",
    "    for art in target:\n",
    "        new_sent = lemmatize_sentence(art)\n",
    "        new_sent = [word for word in new_sent if word not in punc]\n",
    "        new_sent = [word.lower() for word in new_sent]\n",
    "        with_stop_sent = new_sent\n",
    "        \n",
    "        new_sent = clean(new_sent)\n",
    "        new_sent = [word for word in new_sent if word not in stop_words] #\n",
    "\n",
    "        key_index2 = [j for j,x in enumerate(with_stop_sent) if x.find(keyword)!=-1][0]\n",
    "        key_index =[j for j,x in enumerate(new_sent) if x.find(keyword)!=-1][0]\n",
    "\n",
    "        key_art = new_sent[(key_index-n):key_index] + new_sent[(key_index):(key_index+n+1)]\n",
    "        key_collo = with_stop_sent[key_index2-5:key_index2+6] \n",
    "\n",
    "        KeyWord_art.append(key_art)\n",
    "        collocation.append(key_collo)\n",
    "\n",
    "    '''\n",
    "    return collocation, KeyWord_art\n",
    "\n",
    "# extract surrounding texts for a word, n is window size\n",
    "def surround_context(keyword,corpus,n):\n",
    "    instance_list = []\n",
    "    for sent in sentence_corpus:\n",
    "        key_index =[j for j,x in enumerate(sent) if x == keyword]\n",
    "        if len(key_index) != 0:\n",
    "            i = key_index[0]   \n",
    "            new_list = sent[(i-n):i] + sent[(i):(i+n+1)]\n",
    "            instance_list.append(new_list)        \n",
    "    return instance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "noun_key_value = re.compile(r'[a-z]+[%][\\d]+[:][\\d]+[:][\\d]+[:]+[a-z]*[:]*[\\d\\d]*')\n",
    "def RE(S):\n",
    "    tag = re.findall('\\d\\d+', S)[0]  # extraction number\n",
    "    collo = re.sub(' [0-9]+','',S)  #  sub number\n",
    "    return tag, collo\n",
    "\n",
    "def RE2(S):\n",
    "    tag = re.search(noun_key_value, S).group()\n",
    "    collo = re.sub(noun_key_value,'',S)\n",
    "    return tag, collo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为目标单词在corpus中找短语, \n",
    "# param: TEXT-> corpus sentecne list, Word-> target word, \n",
    "# a,b-> start end index in gloss dict, n-> n pairs for similarity compute\n",
    "# T -> threshold score of similarity\n",
    "\n",
    "def tag_collo(TEXT,Word,a,b,n,T):\n",
    "    score_list = []\n",
    "    answer_key_list = []\n",
    "    Score = []\n",
    "    L_colo = []\n",
    "    i=0\n",
    "    while i in range(len(TEXT)):\n",
    "        sim_score = []\n",
    "        for d in EPGwith[a:b]:\n",
    "            sim = SelectWordsSim(d['gloss'], TEXT[i], n) # ith gloss, ith text, nwords\n",
    "            sim_score.append(sim)\n",
    "        max_index = sim_score.index(max(sim_score))\n",
    "        max_score_key = EPGwith[a:b][max_index]['key']  # change dict\n",
    "        gloss_words = EPGwith[a:b][max_index]['gloss']  # change dict\n",
    "        max_score = max(sim_score)\n",
    "        dict={}.fromkeys(('collocation','tag','gloss')) \n",
    "        word_index = [j for j,x in enumerate(TEXT[i]) if x.find(Word)!=-1]\n",
    "        if len(word_index)>0: \n",
    "            word_index = word_index[0]\n",
    "            try:\n",
    "                COLLO = TEXT[i][word_index-2:word_index+3]\n",
    "                if max_score > T and len(COLLO)!=0:\n",
    "                    score_list.append(max_score)\n",
    "                    answer_key_list.append(str(max_score_key))\n",
    "                    Score.append(max_score) \n",
    "                    dict['collocation'] = COLLO\n",
    "                    dict['tag'] = max_score_key\n",
    "                    dict['gloss'] = gloss_words \n",
    "                    L_colo.append(dict)\n",
    "                i = i+1\n",
    "            except Exception as e:\n",
    "                i = i+1\n",
    "        else:\n",
    "            i = i+1\n",
    "       \n",
    "    return L_colo,Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tag_collo_with_filter(word, corpus, n, gs, ge, T, PATTERN):\n",
    "    SC = surround_context(word, corpus, n)\n",
    "    SC_LEM = []\n",
    "    for i in SC:\n",
    "        SC_txt = ' '.join(i)\n",
    "        SC_lem = lemmatize_sentence(SC_txt)\n",
    "        SC_lem = clean(SC_lem)\n",
    "        SC_LEM.append(SC_lem)\n",
    "        \n",
    "    collocations = tag_collo(SC_LEM, word, gs, ge, 8, T)[0]\n",
    "    c_ = []\n",
    "    for i in collocations:\n",
    "        c_tag_d = {}\n",
    "        a = i['collocation']\n",
    "        a_tag = i['tag']\n",
    "        if len(a) == 5:\n",
    "            try:\n",
    "                a.index(word)\n",
    "            except:\n",
    "                continue\n",
    "            else:\n",
    "                c1, c2, c3, c4, c5 = a[:3], a[1:3], a[1:4], a[2:4], a[2:]\n",
    "                if len(c1)>=2: c_tag_d[' '.join(c5)] = a_tag\n",
    "                if len(c2)>=2: c_tag_d[' '.join(c4)] = a_tag\n",
    "                if len(c3)>=2: c_tag_d[' '.join(c3)] = a_tag\n",
    "                if len(c4)>=2: c_tag_d[' '.join(c2)] = a_tag\n",
    "                if len(c5)>=2: c_tag_d[' '.join(c1)] = a_tag\n",
    "            c_.append(c_tag_d)\n",
    "            \n",
    "    list1 = []     # list1 is collocations without tag\n",
    "    list2 = []     # list2 is collocations with tag\n",
    "    for i in range(len(c_)):\n",
    "        for k,v in c_[i].items():\n",
    "            c = k+' '+v\n",
    "            list1.append(k)\n",
    "            list2.append(c)\n",
    "    a=Counter(list1)\n",
    "    b=Counter(list2)\n",
    "    left = []\n",
    "    for k,v in a.items():\n",
    "        for k1,v1 in b.items():\n",
    "            k_sub = PATTERN(k1)[1]\n",
    "            if k == k_sub:\n",
    "            #if k+' '==k_sub:   # the end of noun collocation with a space\n",
    "                left.append((k,v,k1,v1))\n",
    "        \n",
    "    def takeSecond(elem): # Sort by second element\n",
    "        return elem[1]  \n",
    "    left.sort(key=takeSecond, reverse=True)\n",
    "    \n",
    "    #LEFT = []\n",
    "    #for i in left:\n",
    "    #  if i[1]>=5 and i[3]/i[1] >= 0.6:\n",
    "    #    LEFT.append(i[2])\n",
    "\n",
    "    # new rate compute method = p(s|c)/p(s)\n",
    "    key_list = []\n",
    "    for ele in left:\n",
    "        key = PATTERN(ele[2])[0]\n",
    "        key_list.append(key)\n",
    "    cont = Counter(key_list) # get all sense and make a dict\n",
    "    \n",
    "    LEFT=[]\n",
    "    scores = []\n",
    "    for ele in left:\n",
    "        p_s_c = ele[3]/ele[1]\n",
    "        key = PATTERN(ele[2])[0]\n",
    "        sense_count = cont[key]\n",
    "        p_s = sense_count/sum(cont.values())\n",
    "        rate = p_s_c/p_s\n",
    "        \n",
    "        if rate >= 1 and ele[3]>=4:  # 1.0 and 3 is good (Noun) | 1 and 4 for verb [windows=4, threshold=0.45]\n",
    "            LEFT.append(ele[2]) \n",
    " \n",
    "    DICT = {}\n",
    "    for i,j in map(PATTERN, LEFT):    #v-> RE, n-> RE2\n",
    "        DICT[j] = i\n",
    "\n",
    "    verb_delted_list = [word+' what','n\\'t '+word, word+' but', 'but '+word, word+' or','or '+word,'the '+word, word+' the', 'in '+word, 'a '+word, word+' a', word+' that', 'that '+word, word+' it', 'it '+word]\n",
    "    noun_delted_list = ['an '+word+' ', word+' would ', 'be '+word+' ', word+' he ', word+' this ', 'this '+word+' ', word+ ' be ', 'or '+word+' ', word+ ' i ', 'any '+word+' ', 'the '+word+' the ', 'it '+word+' ',word+ ' his ',word+' or ', word+' but ',word+' that ', 'the '+word+' ', word+' the ', 'in '+word+' ', word+' in ', 'at '+'word'+' ', 'for '+word+' ', word+' at ', word+' for ', 'it '+word+' ', word+' it ', 'a '+word+' ', word+' a ', word+' or '] \n",
    "    \n",
    "    for i in noun_delted_list:\n",
    "        try:\n",
    "            del DICT[i]\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    return DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## weighted sim\n",
    "from numpy import *\n",
    "from gensim import matutils\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "myp = '/home_lab_local/s1810410/WSD/stand/stanford-parser-4.0.0/stanford-parser-4.0.0-models.jar'\n",
    "myp2 = '/home_lab_local/s1810410/WSD/stand/stanford-parser-4.0.0/stanford-parser.jar'\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar=myp2, path_to_models_jar=myp)\n",
    "\n",
    "def find_depend_words(target,sent):\n",
    "    # input 一个句子， 一个目标单词\n",
    "    # 返回 这个句子中 和目标单词有依存关系的单词列表\n",
    "    res = dependency_parser.raw_parse(sent)\n",
    "    tree = list(res.__next__().triples())\n",
    "    depend_words = []\n",
    "    for child in tree:\n",
    "        temp = []\n",
    "        for i in child:\n",
    "            temp.append(i[0])\n",
    "        if target in temp:\n",
    "            for word in temp:\n",
    "                if word != target and word in word_vectors.wv.vocab:\n",
    "                    depend_words.append(word)\n",
    "    return depend_words\n",
    " \n",
    "def weighted_sim(sent1, sent2, weight_words): \n",
    "    # 输入两个句子， 权重单词列表\n",
    "    # 返回加权之后的 余弦相似度\n",
    "    sent1 = [word for word in sent1 if word in word_vectors.wv.vocab]\n",
    "    sent2 = [word for word in sent2 if word in word_vectors.wv.vocab]\n",
    "    \n",
    "    v1 = [word_vectors[i]*1.5 if i in weight_words else word_vectors[i] for i in sent1] #如果单词在权重单词中*1.5， 否则不加权重\n",
    "    v2 = [word_vectors[i] for i in sent2]\n",
    "    return dot(matutils.unitvec(array(v1).mean(axis=0)), matutils.unitvec(array(v2).mean(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "####---over---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算相似度（topn pairs方法）\n",
    "import heapq\n",
    "def SelectWordsSim(Glo,Cont,n):\n",
    "    total = []\n",
    "    cont_word = []\n",
    "    for word1 in Glo:\n",
    "        for word2 in Cont:\n",
    "            sim = word_vectors.similarity(word1,word2)\n",
    "            cont_word.append(word2)\n",
    "            total.append(sim)\n",
    "    topn = heapq.nlargest(n,total)\n",
    "    new_words = []\n",
    "    for i in topn:\n",
    "        index = total.index(i)\n",
    "        selected_word = cont_word[index]\n",
    "        new_words.append(selected_word)\n",
    "    new_score = word_vectors.n_similarity(Glo,new_words)\n",
    "    return new_score\n",
    "\n",
    "# if no collocation has been matched, tag sense-key by computing\n",
    "# flag =1 原版计算方法, flag=0 加权计算\n",
    "def Tag_origi(article,gloss, Keyword, flag): \n",
    "    scores = []\n",
    "    for i in range(len(gloss)):\n",
    "        glo = gloss[i]['gloss']\n",
    "        if flag==1:\n",
    "            sim = SelectWordsSim(glo, article, 8) #sim =  word_vectors.n_similarity(glo,article)  # SelectWordsSim(glo, article, 8)\n",
    "        elif flag==0:\n",
    "            sent = ' '.join(article)\n",
    "            weight_words = find_depend_words(Keyword, sent)\n",
    "            sim = weighted_sim(article, glo, weight_words)\n",
    "        scores.append(sim)\n",
    "        index = scores.index(max(scores))\n",
    "        tag = gloss[index]['key']\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最终整合函数！！！！\n",
    "def Compute_(Keyword, Collocations, Articles, GS, GE, F):\n",
    "    target_texts=[]\n",
    "    Colla=[]\n",
    "    DataColla=[]\n",
    "    DATA=()\n",
    "    DataArt=[]\n",
    "    Colla = Collocations \n",
    "    DataColla = Articles[1]  # 0 collo in test data for target word\n",
    "    DataArt = Articles[1]    # 1 for orig; 0 for weighted sim \n",
    "    DataArt2 = Articles[1]\n",
    "    \n",
    "    AN = []   \n",
    "    colloAN =[]  \n",
    "    record_id = []\n",
    "   \n",
    "    if Colla != None:\n",
    "        for i,text in enumerate(DataArt):\n",
    "            flag = False\n",
    "            t1 = ' '.join(text[3:6])\n",
    "            t2 = ' '.join(text[4:6])\n",
    "            t3 = ' '.join(text[4:7])\n",
    "            t4 = ' '.join(text[5:7])\n",
    "            t5 = ' '.join(text[5:8])\n",
    "            t_all = ' '.join(text[3:8])\n",
    "            for k,v in Colla.items():\n",
    "                #if t1+' '==k or t2+' '==k or t3+' '==k or t4+' '==k or t5+' '==k:  # noun & adj\n",
    "                if t1==k or t2==k or t3==k or t4==k or t5==k:                      # verb\n",
    "                    AN.append(v)\n",
    "                    colloAN.append(v)\n",
    "                    record_id.append(i)\n",
    "                    flag = True\n",
    "                    break\n",
    "            if flag == True:\n",
    "                continue\n",
    "            else:\n",
    "                Tag = Tag_origi(DataArt[i], glossDict[GS:GE], Keyword, F)  ## change dict!!  between verb and noun(glossAND_stop)\n",
    "                AN.append(Tag)\n",
    "    else:\n",
    "        for i,text in enumerate(DataArt):\n",
    "            try:\n",
    "                Tag = Tag_origi(DataArt[i], glossDict[GS:GE], Keyword, F)  ## change dict!!  between verb and noun\n",
    "            except Exception as e:\n",
    "                Tag = Tag_origi(DataArt2[i], glossDict[GS:GE], Keyword, 1)\n",
    "            AN.append(Tag)\n",
    "    return AN, colloAN, record_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BATCH_compute(keyword, GS, GE, T, AS, AE):\n",
    "    collocations, articles, answer = None, None, None \n",
    "    #collocations = Tag_collo_with_filter(keyword, sentence_corpus, 8, GS,GE,T, RE) \n",
    "    articles = Find_collo(keyword, AS, AE, 9)       # 5  for noun and verb  \n",
    "    answer = Compute_(keyword, collocations, articles, GS, GE, 1)\n",
    "    return answer[0], answer[1], answer[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all acc: 0.5445859872611465 \n",
      " success\n"
     ]
    }
   ],
   "source": [
    "verb_file = 'corpus/verb.txt'\n",
    "noun_file = 'corpus/noun.txt'\n",
    "adj_file = 'corpus/adj.txt'\n",
    "\n",
    "f_true = pd.read_csv('corpus/v_true.txt',sep=' ',names=['0','1','2','3','4'], header=None, engine='python')\n",
    "f_noun_true = pd.read_csv('corpus/N_true.txt',sep=' ',names=['0','1','2','3','4'], header=None ,engine='python')\n",
    "f_adj_true = pd.read_csv('corpus/all_adj_answer.txt',sep=' ',names=['0','1','2','3','4'],header=None,engine='python')\n",
    "##-\n",
    "f_answer = pd.read_csv(verb_file, sep=' ', header=None, dtype={0:str})  #修改第0列数据类型为str\n",
    "f_noun_answer = pd.read_csv(noun_file,sep =' ',header=None)\n",
    "f_adj_answer = pd.read_csv(adj_file,sep = ' ',header=None)\n",
    "sense_merge()\n",
    "\n",
    "VERB_id = []\n",
    "NOUN_id = []\n",
    "ADJ_id = []\n",
    "j = 0\n",
    "for i in LS_n:           #  LS_n\n",
    "    id = i[1] - i[0]\n",
    "    a = [j, id+j]\n",
    "    NOUN_id.append(a)    # NOUN_id\n",
    "    j = id+j\n",
    "    \n",
    "print('all acc:',newcompare(f_noun_true, f_noun_answer)/1570, '\\n', 'success')   # verb:1698 noun:1570"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all acc: 0.5871613663133097 \n",
      " success\n"
     ]
    }
   ],
   "source": [
    "VERB_id = []\n",
    "j = 0\n",
    "for i in LS_v:           #  LS_n\n",
    "    id = i[1] - i[0]\n",
    "    a = [j, id+j]\n",
    "    VERB_id.append(a)    # NOUN_id\n",
    "    j = id+j\n",
    "    \n",
    "print('all acc:',newcompare(f_true, f_answer)/1698, '\\n', 'success')   # verb:1698 noun:1570"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all acc: 0.5106382978723404 \n",
      " success\n"
     ]
    }
   ],
   "source": [
    "ADJ_id = []\n",
    "j = 0\n",
    "for i in LS_adj:           #  LS_n\n",
    "    id = i[1] - i[0]\n",
    "    a = [j, id+j]\n",
    "    ADJ_id.append(a)    # NOUN_id\n",
    "    j = id+j\n",
    "    \n",
    "print('all acc:',newcompare(f_adj_true, f_adj_answer)/141, '\\n', 'success')   # verb:1698 noun:1570"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "a = !ls cd ./dependency/new*\n",
    "file_list = a[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./dependency/new_dependency_activate.txt'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verb : 101742 -> 297\n",
    "# noun : 25151 -> 187\n",
    "# adj: 5407 -> 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n"
     ]
    }
   ],
   "source": [
    "# 查看 过滤后数量\n",
    "n = 0\n",
    "for k, file in enumerate(file_list):\n",
    "    tem = filter_count(path, LS_v, k)\n",
    "    n += tem\n",
    "print(n)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dependency/new_dependency_activate.txt successed!\n",
      "./dependency/new_dependency_add.txt successed!\n",
      "./dependency/new_dependency_appear.txt successed!\n",
      "./dependency/new_dependency_ask.txt successed!\n",
      "./dependency/new_dependency_begin.txt successed!\n",
      "./dependency/new_dependency_climb.txt successed!\n",
      "./dependency/new_dependency_eat.txt successed!\n",
      "./dependency/new_dependency_encounter.txt successed!\n",
      "./dependency/new_dependency_hear.txt successed!\n",
      "./dependency/new_dependency_lose.txt successed!\n",
      "./dependency/new_dependency_mean.txt successed!\n",
      "./dependency/new_dependency_miss.txt successed!\n",
      "./dependency/new_dependency_play.txt successed!\n",
      "./dependency/new_dependency_produce.txt successed!\n",
      "./dependency/new_dependency_provide.txt successed!\n",
      "./dependency/new_dependency_receive.txt successed!\n",
      "./dependency/new_dependency_remain.txt successed!\n",
      "./dependency/new_dependency_rule.txt successed!\n",
      "./dependency/new_dependency_smell.txt successed!\n",
      "./dependency/new_dependency_suspend.txt successed!\n",
      "./dependency/new_dependency_talk.txt successed!\n",
      "./dependency/new_dependency_treat.txt successed!\n",
      "./dependency/new_dependency_use.txt successed!\n",
      "./dependency/new_dependency_wash.txt successed!\n",
      "./dependency/new_dependency_watch.txt successed!\n",
      "./dependency/new_dependency_win.txt successed!\n",
      "./dependency/new_dependency_write.txt successed!\n",
      "Exe time---: 8.538692916666665 min\n"
     ]
    }
   ],
   "source": [
    "# 更换答案\n",
    "import time\n",
    "start = time.clock()\n",
    "new_collo = []\n",
    "temp = []\n",
    "for k,file in enumerate(file_list):\n",
    "    path = file\n",
    "    #s, e = NOUN_id[k][0], NOUN_id[k][1]  #\n",
    "    s, e = VERB_id[k][0], VERB_id[k][1]  #\n",
    "    #s, e = ADJ_id[k][0], ADJ_id[k][1]  #\n",
    "    \n",
    "    test_new = GetQualtity(path, LS_v, k) # CHANGE \n",
    "    play = pd.DataFrame(test_new[0])\n",
    "    temp.append(test_new[0])    \n",
    "    print(file + ' successed!') \n",
    "    \n",
    "sense_merge()\n",
    "end = time.clock()\n",
    "print('Exe time---:', (end-start)/60,'min')\n",
    "#print(NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verb(depd collo 482)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1698\n",
      "482\n"
     ]
    }
   ],
   "source": [
    "#temp \n",
    "new_temp = []\n",
    "for i in temp:\n",
    "    for e in i:\n",
    "        new_temp.append(e)\n",
    "print(len(new_temp))\n",
    "\n",
    "id_tag_with_collo = []\n",
    "num = 0\n",
    "for i in range(f_answer.shape[0]): # change\n",
    "    if new_temp[i] != ' ':\n",
    "        id_tag_with_collo.append(i)\n",
    "        f_answer[0][i] = new_temp[i]  # change\n",
    "        num += 1\n",
    "sense_merge()\n",
    "print(num) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221\n"
     ]
    }
   ],
   "source": [
    "noun_collo_id = [14, 15, 23, 49, 112, 118, 120, 121, 122, 123, 125, 126, 127, 133, 135, 139, 140, 141, 142, 144, 146, 151, 152, 153, 154, 158, 159, 164, 168, 169, 205, 208, 212, 217, 219, 224, 231, 240, 241, 243, 622, 629, 634, 637, 826, 841, 853, 859, 860, 877, 883, 892, 893, 898, 900, 901, 907, 908, 909, 918, 934, 939, 1443, 1459, 1500, 1505, 1506, 1531]\n",
    "adj_collo_id = [5, 26, 38, 95, 100, 109]\n",
    "verb_collo_id = [121, 130, 158, 205, 207, 208, 215, 227, 234, 235, 239, 240, 380, 381, 383, 384, 385, 386, 387, 388, 390, 391, 393, 394, 395, 396, 397, 398, 399, 400, 401, 403, 404, 405, 406, 407, 415, 418, 419, 420, 421, 429, 433, 434, 435, 436, 437, 438, 441, 442, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 476, 477, 478, 479, 480, 482, 483, 484, 485, 488, 489, 490, 491, 492, 494, 495, 498, 500, 503, 504, 505, 506, 507, 508, 509, 661, 664, 669, 670, 672, 683, 687, 689, 691, 693, 694, 695, 697, 698, 701, 704, 705, 708, 712, 717, 723, 725, 728, 734, 811, 813, 815, 823, 825, 833, 836, 837, 839, 841, 843, 844, 849, 851, 860, 863, 872, 957, 971, 977, 982, 993, 1007, 1009, 1011, 1014, 1016, 1020, 1023, 1034, 1039, 1042, 1043, 1044, 1045, 1049, 1050, 1051, 1052, 1061, 1065, 1066, 1069, 1071, 1073, 1075, 1081, 1082, 1089, 1090, 1096, 1098, 1130, 1141, 1155, 1290, 1293, 1295, 1296, 1299, 1315, 1317, 1318, 1322, 1329, 1330, 1331, 1332, 1336, 1539, 1543, 1549, 1636, 1639, 1641, 1642, 1646, 1647, 1648, 1649, 1651, 1652, 1653, 1656, 1661, 1663, 1664, 1666, 1668, 1671, 1672, 1673]\n",
    "\n",
    "print(len(verb_collo_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verb all collo id\n",
    "all_collo = id_tag_with_collo + verb_collo_id\n",
    "all_collo = set(all_collo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "618 0.5728155339805825\n"
     ]
    }
   ],
   "source": [
    "co_num = 0\n",
    "for i in all_collo:\n",
    "    tans = f_true.iloc[i].values.tolist()[2:]\n",
    "    ans = f_answer.iloc[i].values\n",
    "    if ans[0] in tans:\n",
    "        co_num += 1\n",
    "\n",
    "print(len(all_collo), co_num / len(all_collo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "0.5942\n"
     ]
    }
   ],
   "source": [
    "# 显示单词正确率，整体正确率\n",
    "def temo_comp(ID, true, answer, pos):\n",
    "    ret =  {}\n",
    "    all_counts = []\n",
    "    all_data = []\n",
    "    for k,i in enumerate(ID):\n",
    "        word = pos[k][2]\n",
    "        counts = 0\n",
    "        l, r = i[0], i[1]\n",
    "        subt = true.iloc[l:r]\n",
    "        suba = answer.iloc[l:r]\n",
    "        total = subt.shape[0]\n",
    "        for n in range(total):\n",
    "            if suba.iloc[n][0] in subt.iloc[n][2:5].values.tolist():\n",
    "                counts+=1\n",
    "        acc = round(counts/total , 4)\n",
    "        all_counts.append(counts)\n",
    "        all_data.append(total)\n",
    "        ret[word] = acc\n",
    "    all_acc = round(sum(all_counts)/sum(all_data), 4)\n",
    "    fin = sorted(ret.items(), key=lambda x:x[1], reverse=True)\n",
    "    return fin, all_acc\n",
    "result = temo_comp(VERB_id, f_true, f_answer, LS_v)\n",
    "print('--------------')\n",
    "print(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, n , a = 1698, 1570, 141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all acc: 0.5471142520612485\n"
     ]
    }
   ],
   "source": [
    "VERB_id = []\n",
    "NOUN_id = []\n",
    "ADJ_id = []\n",
    "j = 0\n",
    "for i in LS_v:    # LS_n\n",
    "    id = i[1] - i[0]\n",
    "    a = [j, id+j]\n",
    "    VERB_id.append(a)# NOUN_id\n",
    "    j = id+j\n",
    "\n",
    "print('all acc:',newcompare(f_noun_true, f_noun_answer)/15)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all acc: 0.5942285041224971\n"
     ]
    }
   ],
   "source": [
    "print('all acc:',newcompare(f_true, f_answer)/1698)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser acc verb 0.561\n",
    "# parse acc noun 0.541\n",
    "# parser acc verb 0.541\n",
    "\n",
    "def AccSort(ID, TRUE, ANSWER, POS):\n",
    "    #计算单个单词的精确度，观察固定搭配的熵是否有影响\n",
    "    ret = []\n",
    "    _,Colcorrect,Collen = comp_single(ID, TRUE, ANSWER)\n",
    "    for k,i in enumerate(ID):\n",
    "        word = POS[k][2]\n",
    "        l,r = i[0], i[1]\n",
    "        length = r-l\n",
    "        res = newcompare(TRUE[l:r], ANSWER[l:r])/length         #1570  # accuracy for all noun instances\n",
    "\n",
    "        collo_acc = Colcorrect[k] / (Collen[k]+0.001)\n",
    "        rate = Collen[k] / length\n",
    "        #ret.append([word, round(res,2), round(collo_acc,2), round(rate, 2)])\n",
    "        ret.append([word, round(res, 2)])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def compute_acc(index, tru, ans):   \n",
    "    # 计算每个单词的collo的acc\n",
    "    LEN_collo = []\n",
    "    correct_number = []\n",
    "    for i in range(len(id_tag_with_collo)):\n",
    "        INDEX = id_tag_with_collo[i]\n",
    "        A = index[i] ##\n",
    "        c = colloComapre(tru, ans, A[0], A[1], INDEX)  ##\n",
    "        LEN_collo.append(len(INDEX))\n",
    "        correct_number.append(c)\n",
    "    if sum(LEN_collo) != 0:\n",
    "        return sum(LEN_collo), sum(correct_number)/sum(LEN_collo)  # p for tagged by collo insatnces\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def newcompare(TRUE,ANSWER):\n",
    "    correct = []\n",
    "    for i in range(len(ANSWER)):\n",
    "        if ANSWER.iloc[i][0] in TRUE.iloc[i][2:5].values:\n",
    "            correct.append(1)\n",
    "    return len(correct)\n",
    "\n",
    "def colloComapre(TRUE, ANSWER, AS, AE, INDEX):\n",
    "    newTrue = None\n",
    "    newAnswer = None\n",
    "    number = None\n",
    "\n",
    "    newTrue = TRUE[AS:AE]\n",
    "    newAnswer = ANSWER[AS:AE]\n",
    "    number = []\n",
    "    if len(INDEX) == 0:\n",
    "        return 0\n",
    "    else:    \n",
    "        for index in INDEX:\n",
    "            answer = newAnswer.iloc[index][0]\n",
    "            Tr = newTrue.iloc[index][2:5].values\n",
    "            if answer in Tr:\n",
    "                number.append(1)\n",
    "    return len(number)\n",
    "\n",
    "\n",
    "def comp_single(ID, f_tr, f_an):\n",
    "    LEN_collo = []\n",
    "    correct_number = []\n",
    "    \n",
    "    for i in range(len(id_tag_with_collo)):\n",
    "        INDEX = id_tag_with_collo[i]\n",
    "        A = ID[i] ##\n",
    "        c = colloComapre(f_tr, f_an, A[0], A[1], INDEX)  ##\n",
    "        LEN_collo.append(len(INDEX))\n",
    "        correct_number.append(c)\n",
    "    \n",
    "    acc = []\n",
    "    for idx, i in enumerate(LEN_collo):\n",
    "        if i != 0:\n",
    "            ac = correct_number[idx]/i\n",
    "            acc.append(ac)\n",
    "        else:\n",
    "            acc.append(0)\n",
    "    return acc, correct_number, LEN_collo # CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_count(jsonfile, LS, i):\n",
    "    LS = LS[i]\n",
    "    \n",
    "    # get quality collo by filt-score\n",
    "    with open(jsonfile, 'r') as f:\n",
    "        rawdata = f.readlines()\n",
    "        rawdata = [i.strip('\\n') for i in rawdata]\n",
    "        rawdata = [i.split(' ') for i in rawdata]\n",
    "        data = []\n",
    "        for i in rawdata:\n",
    "            collo = i[0] + ' ' + i[1] + ' ' + i[3]\n",
    "            sense = i[4]\n",
    "            data.append((collo, sense))\n",
    "\n",
    "    a = {}\n",
    "    b = {}\n",
    "    for item in data:\n",
    "        co = item[0]\n",
    "        se = item[1]\n",
    "        if co not in a:\n",
    "            a[co] = 1\n",
    "        else:\n",
    "            a[co] += 1\n",
    "\n",
    "        co_se = co + ' ' + se\n",
    "        if co_se not in b:\n",
    "            b[co_se] = 1\n",
    "        else:\n",
    "            b[co_se] += 1\n",
    "\n",
    "    reliable = {}\n",
    "    for c,n in b.items():\n",
    "        k = RE(c)[1]  # RE -> V  ; RE2 -> N\n",
    "        #k = k.strip()  # N -> remove space\n",
    "        if k in skip:\n",
    "            continue\n",
    "        else:\n",
    "            if k in a:\n",
    "                appear_n = n\n",
    "                score = n/a[k]\n",
    "                if appear_n > 5 and score >= 0.7:    # (appear_n 5, score 0.7)\n",
    "                    sense = c.split(' ')[-1]\n",
    "                    reliable[k] = sense\n",
    "    return len(reliable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_collo_id = [121, 130, 158, 205, 207, 208, 215, 227, 234, 235, 239, 240, 380, 381, 383, 384, 385, 386, 387, 388, 390, 391, 393, 394, 395, 396, 397, 399, 400, 401, 403, 404, 405, 406, 407, 415, 418, 419, 420, 421, 429, 433, 434, 435, 436, 437, 438, 441, 442, 444, 445, 446, 447, 448, 449, 450, 451, 452, 454, 455, 456, 457, 458, 460, 461, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 476, 477, 478, 479, 480, 482, 483, 484, 485, 488, 489, 490, 491, 492, 495, 498, 500, 503, 504, 505, 506, 507, 508, 509, 669, 670, 672, 683, 687, 689, 691, 693, 694, 695, 697, 698, 701, 704, 705, 712, 717, 723, 725, 728, 843, 844, 849, 851, 860, 863, 957, 971, 977, 982, 993, 1007, 1009, 1011, 1014, 1016, 1020, 1023, 1034, 1039, 1042, 1043, 1044, 1045, 1049, 1050, 1051, 1052, 1061, 1065, 1066, 1069, 1071, 1073, 1075, 1081, 1082, 1089, 1090, 1096, 1098, 1130, 1141, 1155, 1290, 1295, 1296, 1299, 1317, 1329, 1330, 1331, 1332, 1636, 1639, 1641, 1642, 1646, 1647, 1648, 1649, 1651, 1652, 1653, 1656, 1661, 1663, 1664, 1666, 1668, 1671, 1672, 1673]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun -> dict expand with stop word; sim T: 0.7, window size: 5, Top n: 8, rate>=1 occur>=3\n",
    "# acc: 0.794; 34个; all with acc: 0.52\n",
    "# sim T: 0.65, acc 0.489; 133个, all with acc 0.518\n",
    "# sim T: 0.6, acc 0.511; 233 个, all with acc 0.52\n",
    "# sim T: 0.5, acc 0.498; 291个, all with acc 0.514\n",
    "\n",
    "#newcompare(f_noun_true, f_noun_answer)/1570\n",
    "#newcompare(f_true, f_answer)/1698     #, newcompare(f_adj_true, f_adj_answer)/141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LS_v = [(0,114,'activate',0,5),(114,246,'add',5,11),(246,379,'appear',11,14),(623,754,'ask',25,31),\n",
    "     \n",
    "        (1067,1146,'begin',51,55),(1146,1213,'climb',55,60),(1690,1777,'eat',89,96),\n",
    "      \n",
    "        (1777,1842,'encounter',96,100),\n",
    "      \n",
    "        (1975,2007,'hear',107,114),(2268,2304,'lose',162,171),(2304,2344,'mean',171,178),(2344,2374,'miss',178,186)\n",
    "     \n",
    "        ,(2919,2971,'play',221,233),(2971,3065,'produce',233,239),(3065,3134,'provide',239,245),\n",
    "     \n",
    "        (3134,3161,'receive',245,254),(3161,3231,'remain',254,257),(3231,3261,'rule',257,262),(3377,3432,'smell',273,280),\n",
    "     \n",
    "        (3589,3653,'suspend',305,312),(3653,3726,'talk',312,321),(3726,3783,'treat',321,330),(3783,3797,'use',330,335),\n",
    "     \n",
    "        (3797,3831,'wash',335,347),(3831,3882,'watch',347,354),(3882,3921,'win',354,361),(3921,3944,'write',361,369)]\n",
    "      \n",
    "\n",
    "LS_n = [(379,490,'argument',14,19),(490,623,'arm',19,25),\n",
    "         \n",
    "         (935,1067,'bank',41,51),(1275,1403,'degree',64,71),(1403,1517,'difference',71,76),(1567,1590,'difficulty',81,85),\n",
    "         \n",
    "         (1590,1690,'disc',85,89),(2050,2124,'image',136,143),(2143,2236,'interest',148,155),(2236,2268,'judgment',155,162),\n",
    "         (2515,2632,'paper',201,208),(2632,2748,'party',208,213),(2748,2835,'performance',213,218),\n",
    "         \n",
    "         (2835,2919,'plan',218,221),(3261,3359,'shelter',262,266),(3461,3557,'sort',294,298),(3557,3589,'source',298,305)]\n",
    " \n",
    "\n",
    "LS_adj = [(1517,1567,'different',76,81),(2007,2050,'hot',114,136),\n",
    "          (2124,2143,'important',143,148),(3432,3461,'solid',280,294)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noune  合并后平均意思3.58， 17个单词， 1570个例子\n",
    "def sense_merge():\n",
    "    f_noun_answer.replace('argument%1:10:00::','a',inplace=True)\n",
    "    f_noun_answer.replace('argument%1:10:00::','a',inplace=True)\n",
    "    f_noun_answer.replace('bank%1:06:00::','b',inplace=True)\n",
    "    f_noun_answer.replace('bank%1:06:01::','b',inplace=True)\n",
    "    f_noun_answer.replace('bank%1:14:00::','b',inplace=True)\n",
    "    f_noun_answer.replace('bank%1:14:01::','b',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('bank%1:17:00::','c',inplace=True)\n",
    "    f_noun_answer.replace('bank%1:17:01::','c',inplace=True)\n",
    "    f_noun_answer.replace('bank%1:17:02::','c',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('bank%1:21:00::','d',inplace=True)\n",
    "    f_noun_answer.replace('bank%1:21:01::','d',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('degree%1:07:00::','e',inplace=True)\n",
    "    f_noun_answer.replace('degree%1:07:01::','e',inplace=True)\n",
    "    f_noun_answer.replace('degree%1:26:01::','e',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('image%1:06:00::','f',inplace=True)\n",
    "    f_noun_answer.replace('image%1:06:01::','f',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('image%1:07:00::','g',inplace=True)\n",
    "    f_noun_answer.replace('image%1:09:00::','g',inplace=True)\n",
    "    f_noun_answer.replace('image%1:09:02::','g',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('interest%1:04:01::','h',inplace=True)\n",
    "    f_noun_answer.replace('interest%1:07:01::','h',inplace=True)\n",
    "    f_noun_answer.replace('interest%1:07:02::','h',inplace=True)\n",
    "    f_noun_answer.replace('interest%1:09:00::','h',inplace=True)\n",
    "    f_noun_answer.replace('interest%1:14:00::','h',inplace=True)\n",
    "    f_noun_answer.replace('interest%1:21:00::','i',inplace=True)\n",
    "    f_noun_answer.replace('interest%1:21:03::','i',inplace=True)\n",
    "\n",
    "    #------------------\n",
    "    f_noun_answer.replace('judgment%1:04:00::','j',inplace=True)\n",
    "    f_noun_answer.replace('judgment%1:04:02::','j',inplace=True)\n",
    "    f_noun_answer.replace('judgment%1:10:00::','j',inplace=True)\n",
    "    f_noun_answer.replace('judgment%1:07:00::','k',inplace=True)\n",
    "    f_noun_answer.replace('judgment%1:09:00::','k',inplace=True)\n",
    "    f_noun_answer.replace('judgment%1:09:01::','k',inplace=True)\n",
    "    f_noun_answer.replace('judgment%1:09:04::','k',inplace=True)\n",
    "    #----------------\n",
    "\n",
    "    f_noun_answer.replace('paper%1:10:01::','l',inplace=True)\n",
    "    f_noun_answer.replace('paper%1:10:02::','l',inplace=True)\n",
    "    f_noun_answer.replace('paper%1:06:00::','m',inplace=True)\n",
    "    f_noun_answer.replace('paper%1:10:03::','m',inplace=True)\n",
    "    f_noun_answer.replace('paper%1:14:00::','m',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('paper%1:10:00::','n',inplace=True)\n",
    "    f_noun_answer.replace('paper%1:27:00::','n',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('party%1:11:00::','o',inplace=True)\n",
    "    f_noun_answer.replace('party%1:14:02::','o',inplace=True)\n",
    "    f_noun_answer.replace('party%1:18:00::','o',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('performance%1:04:00::','p',inplace=True)\n",
    "    f_noun_answer.replace('performance%1:04:01::','p',inplace=True)\n",
    "    f_noun_answer.replace('performance%1:10:00::','p',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('performance%1:04:03::','q',inplace=True)\n",
    "    f_noun_answer.replace('performance%1:22:00::','q',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('plan%1:09:00::','r',inplace=True)\n",
    "    f_noun_answer.replace('plan%1:09:01::','r',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('shelter%1:06:00::','s',inplace=True)\n",
    "    f_noun_answer.replace('shelter%1:06:01::','s',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('shelter%1:21:00::','t',inplace=True)\n",
    "    f_noun_answer.replace('shelter%1:26:00::','t',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('source%1:10:00::','u',inplace=True)\n",
    "    f_noun_answer.replace('source%1:10:01::','u',inplace=True)\n",
    "    f_noun_answer.replace('source%1:18:00::','u',inplace=True)\n",
    "\n",
    "\n",
    "    #\n",
    "    #answer\n",
    "    #=-----------------\n",
    "    f_noun_true.replace('argument%1:10:00::','a',inplace=True)\n",
    "    f_noun_true.replace('argument%1:10:00::','a',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('bank%1:06:00::','b',inplace=True)\n",
    "    f_noun_true.replace('bank%1:06:01::','b',inplace=True)\n",
    "    f_noun_true.replace('bank%1:14:00::','b',inplace=True)\n",
    "    f_noun_true.replace('bank%1:14:01::','b',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('bank%1:17:00::','c',inplace=True)\n",
    "    f_noun_true.replace('bank%1:17:01::','c',inplace=True)\n",
    "    f_noun_true.replace('bank%1:17:02::','c',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('bank%1:21:00::','d',inplace=True)\n",
    "    f_noun_true.replace('bank%1:21:01::','d',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('degree%1:07:00::','e',inplace=True)\n",
    "    f_noun_true.replace('degree%1:07:01::','e',inplace=True)\n",
    "    f_noun_true.replace('degree%1:26:01::','e',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('image%1:06:00::','f',inplace=True)\n",
    "    f_noun_true.replace('image%1:06:01::','f',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('image%1:07:00::','g',inplace=True)\n",
    "    f_noun_true.replace('image%1:09:00::','g',inplace=True)\n",
    "    f_noun_true.replace('image%1:09:02::','g',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('interest%1:04:01::','h',inplace=True)\n",
    "    f_noun_true.replace('interest%1:07:01::','h',inplace=True)\n",
    "    f_noun_true.replace('interest%1:07:02::','h',inplace=True)\n",
    "    f_noun_true.replace('interest%1:09:00::','h',inplace=True)\n",
    "    f_noun_true.replace('interest%1:14:00::','h',inplace=True)\n",
    "    f_noun_true.replace('interest%1:21:00::','i',inplace=True)\n",
    "    f_noun_true.replace('interest%1:21:03::','i',inplace=True)\n",
    "\n",
    "    #------------------\n",
    "    f_noun_true.replace('judgment%1:04:00::','j',inplace=True)\n",
    "    f_noun_true.replace('judgment%1:04:02::','j',inplace=True)\n",
    "    f_noun_true.replace('judgment%1:10:00::','j',inplace=True)\n",
    "    f_noun_true.replace('judgment%1:07:00::','k',inplace=True)\n",
    "    f_noun_true.replace('judgment%1:09:00::','k',inplace=True)\n",
    "    f_noun_true.replace('judgment%1:09:01::','k',inplace=True)\n",
    "    f_noun_true.replace('judgment%1:09:04::','k',inplace=True)\n",
    "    #----------------\n",
    "\n",
    "    f_noun_true.replace('paper%1:10:01::','l',inplace=True)\n",
    "    f_noun_true.replace('paper%1:10:02::','l',inplace=True)\n",
    "    f_noun_true.replace('paper%1:06:00::','m',inplace=True)\n",
    "    f_noun_true.replace('paper%1:10:03::','m',inplace=True)\n",
    "    f_noun_true.replace('paper%1:14:00::','m',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('paper%1:10:00::','n',inplace=True)\n",
    "    f_noun_true.replace('paper%1:27:00::','n',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('party%1:11:00::','o',inplace=True)\n",
    "    f_noun_true.replace('party%1:14:02::','o',inplace=True)\n",
    "    f_noun_true.replace('party%1:18:00::','o',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('performance%1:04:00::','p',inplace=True)\n",
    "    f_noun_true.replace('performance%1:04:01::','p',inplace=True)\n",
    "    f_noun_true.replace('performance%1:10:00::','p',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('performance%1:04:03::','q',inplace=True)\n",
    "    f_noun_true.replace('performance%1:22:00::','q',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('plan%1:09:00::','r',inplace=True)\n",
    "    f_noun_true.replace('plan%1:09:01::','r',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('shelter%1:06:00::','s',inplace=True)\n",
    "    f_noun_true.replace('shelter%1:06:01::','s',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('shelter%1:21:00::','t',inplace=True)\n",
    "    f_noun_true.replace('shelter%1:26:00::','t',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('source%1:10:00::','u',inplace=True)\n",
    "    f_noun_true.replace('source%1:10:01::','u',inplace=True)\n",
    "    f_noun_true.replace('source%1:18:00::','u',inplace=True)\n",
    "\n",
    "    #Verb  合并后意思 4.07， 27个单词， 1698 个例子\n",
    "\n",
    "    f_answer.replace('42602','a',inplace=True)\n",
    "    f_answer.replace('42605','a',inplace=True)\n",
    "    f_answer.replace('238101','b',inplace=True)\n",
    "    f_answer.replace('238105','b',inplace=True)\n",
    "    f_answer.replace('238102','c',inplace=True)\n",
    "    f_answer.replace('238104','c',inplace=True)\n",
    "    f_answer.replace('238106','c',inplace=True)  # delete 'begin'\n",
    "    f_answer.replace('770001','d',inplace=True)\n",
    "    f_answer.replace('770002','d',inplace=True)   #delete 'decide'\n",
    "    f_answer.replace('770005','d',inplace=True)\n",
    "    f_answer.replace('1297001','e',inplace=True)\n",
    "    f_answer.replace('1297006','e',inplace=True)\n",
    "    f_answer.replace('1297002','f',inplace=True)\n",
    "    f_answer.replace('1297007','f',inplace=True)\n",
    "    f_answer.replace('1353101','g',inplace=True)  #delete expect,express\n",
    "    f_answer.replace('1353103','g',inplace=True)\n",
    "    f_answer.replace('1892101','h',inplace=True)\n",
    "    f_answer.replace('1892103','h',inplace=True)\n",
    "    f_answer.replace('1892105','h',inplace=True)\n",
    "\n",
    "    f_answer.replace('1892102','i',inplace=True)\n",
    "    f_answer.replace('1892104','i',inplace=True)\n",
    "    f_answer.replace('1892106','i',inplace=True)\n",
    "    f_answer.replace('1892107','i',inplace=True)\n",
    "\n",
    "    f_answer.replace('2439901','j',inplace=True)\n",
    "    f_answer.replace('2439902','j',inplace=True)\n",
    "    f_answer.replace('2439904','j',inplace=True)\n",
    "    f_answer.replace('2439905','j',inplace=True)\n",
    "\n",
    "    f_answer.replace('2439903','k',inplace=True)\n",
    "    f_answer.replace('2439908','k',inplace=True)\n",
    "    f_answer.replace('2439906','k',inplace=True)\n",
    "\n",
    "    f_answer.replace('2555501','l',inplace=True)\n",
    "    f_answer.replace('2555502','l',inplace=True)\n",
    "    f_answer.replace('2555507','l',inplace=True)\n",
    "\n",
    "    f_answer.replace('2555504','m',inplace=True)\n",
    "    f_answer.replace('2555505','m',inplace=True)\n",
    "    f_answer.replace('2555506','m',inplace=True)\n",
    "\n",
    "    f_answer.replace('2644301','n',inplace=True)\n",
    "    f_answer.replace('2644307','n',inplace=True)\n",
    "    f_answer.replace('2644302','n',inplace=True)\n",
    "    f_answer.replace('2644303','n',inplace=True)   # delete note\n",
    "\n",
    "    f_answer.replace('3165218','o',inplace=True)\n",
    "    f_answer.replace('3165217','o',inplace=True)\n",
    "    f_answer.replace('3165213','o',inplace=True)\n",
    "    \n",
    "    f_answer.replace('3165220','p',inplace=True)\n",
    "    f_answer.replace('3165214','p',inplace=True)\n",
    "\n",
    "    f_answer.replace('3288301','q',inplace=True)\n",
    "    f_answer.replace('3288306','q',inplace=True)\n",
    "\n",
    "    f_answer.replace('3288302','r',inplace=True)\n",
    "    f_answer.replace('3288305','r',inplace=True)\n",
    "\n",
    "    f_answer.replace('3313901','s',inplace=True)\n",
    "    f_answer.replace('3313906','s',inplace=True)\n",
    "    f_answer.replace('3313902','t',inplace=True)\n",
    "    f_answer.replace('3313905','t',inplace=True)\n",
    "\n",
    "    f_answer.replace('3434801','u',inplace=True)\n",
    "    f_answer.replace('3434806','u',inplace=True)\n",
    "\n",
    "    f_answer.replace('3434803','v',inplace=True)\n",
    "    f_answer.replace('3434807','v',inplace=True)\n",
    "\n",
    "    f_answer.replace('3477801','w',inplace=True)\n",
    "    f_answer.replace('3477803','w',inplace=True)\n",
    "\n",
    "    f_answer.replace('3597906','x',inplace=True)\n",
    "    f_answer.replace('3597907','x',inplace=True)\n",
    "    f_answer.replace('3597911','x',inplace=True)\n",
    "\n",
    "    f_answer.replace('3893501','y',inplace=True)\n",
    "    f_answer.replace('3893505','y',inplace=True)\n",
    "    f_answer.replace('3893507','y',inplace=True)\n",
    "    f_answer.replace('3893508','y',inplace=True)\n",
    "\n",
    "    f_answer.replace('3893502','z',inplace=True)\n",
    "    f_answer.replace('3893503','z',inplace=True)\n",
    "    f_answer.replace('3893509','z',inplace=True)\n",
    "\n",
    "    f_answer.replace('4155302','aa',inplace=True)\n",
    "    f_answer.replace('4155303','aa',inplace=True)\n",
    "    f_answer.replace('4155304','aa',inplace=True)\n",
    "    f_answer.replace('4155307','aa',inplace=True)\n",
    "\n",
    "    f_answer.replace('4198501','bb',inplace=True)\n",
    "    f_answer.replace('4198502','bb',inplace=True)\n",
    "    f_answer.replace('4198503','bb',inplace=True)\n",
    "    f_answer.replace('4198506','bb',inplace=True)\n",
    "    f_answer.replace('4198507','bb',inplace=True)  \n",
    "\n",
    "    f_answer.replace('4380101','cc',inplace=True)\n",
    "    f_answer.replace('4380102','cc',inplace=True)\n",
    "    f_answer.replace('4380103','cc',inplace=True)\n",
    "\n",
    "    f_answer.replace('4380104','dd',inplace=True)\n",
    "    f_answer.replace('4380105','dd',inplace=True)\n",
    "\n",
    "    #--------------------------------------------\n",
    "    f_answer.replace('4380106','ee',inplace=True)\n",
    "    f_answer.replace('4380109','ee',inplace=True)\n",
    "\n",
    "    f_answer.replace('4530702','ff',inplace=True)\n",
    "    f_answer.replace('4530704','ff',inplace=True)\n",
    "\n",
    "    f_answer.replace('4636102','gg',inplace=True) \n",
    "    f_answer.replace('4636101','gg',inplace=True) \n",
    "    f_answer.replace('4636107','gg',inplace=True) \n",
    "    f_answer.replace('4636108','gg',inplace=True) \n",
    "\n",
    "    f_answer.replace('4636103','hh',inplace=True) \n",
    "    f_answer.replace('4636104','hh',inplace=True) \n",
    "    f_answer.replace('4636110','hh',inplace=True) \n",
    "    f_answer.replace('4636111','hh',inplace=True) \n",
    "    f_answer.replace('4636112','hh',inplace=True) \n",
    "    #---------------\n",
    "\n",
    "\n",
    "    f_answer.replace('4640501','ii',inplace=True) \n",
    "    f_answer.replace('4640502','ii',inplace=True) \n",
    "    f_answer.replace('4640507','ii',inplace=True)\n",
    "\n",
    "    f_answer.replace('4640503','jj',inplace=True) \n",
    "    f_answer.replace('4640508','jj',inplace=True) \n",
    "    f_answer.replace('4640509','jj',inplace=True) \n",
    "    #------------------\n",
    "    f_answer.replace('4711401','kk',inplace=True) \n",
    "    f_answer.replace('4711403','kk',inplace=True) \n",
    "    f_answer.replace('4711405','kk',inplace=True)\n",
    "\n",
    "    f_answer.replace('4711404','ll',inplace=True) \n",
    "    f_answer.replace('4711406','ll',inplace=True) \n",
    "\n",
    "    #---------------------------------\n",
    "    #新添加 \n",
    "    f_answer.replace('4753404','mm',inplace=True) \n",
    "    f_answer.replace('4753406','mm',inplace=True)\n",
    "    f_answer.replace('4753408','mm',inplace=True)\n",
    "\n",
    "    f_answer.replace('4753403','nn',inplace=True) \n",
    "    f_answer.replace('4753407','nn',inplace=True)\n",
    "    \n",
    "    f_answer.replace('4753401','oo',inplace=True)\n",
    "    f_answer.replace('4753405','oo',inplace=True) \n",
    "\n",
    "    #\n",
    "    # 答案\n",
    "    #---------------------------------------------------------\n",
    "    f_true.replace('42602','a',inplace=True)\n",
    "    f_true.replace('42605','a',inplace=True)\n",
    "    f_true.replace('238101','b',inplace=True)\n",
    "    f_true.replace('238105','b',inplace=True)\n",
    "    f_true.replace('238102','c',inplace=True)\n",
    "    f_true.replace('238104','c',inplace=True)\n",
    "    f_true.replace('238106','c',inplace=True)  # delete 'begin'\n",
    "    f_true.replace('770001','d',inplace=True)\n",
    "    f_true.replace('770002','d',inplace=True)   #delete 'decide'\n",
    "    f_true.replace('770005','d',inplace=True)\n",
    "    f_true.replace('1297001','e',inplace=True)\n",
    "    f_true.replace('1297006','e',inplace=True)\n",
    "    f_true.replace('1297002','f',inplace=True)\n",
    "    f_true.replace('1297007','f',inplace=True)\n",
    "    f_true.replace('1353101','g',inplace=True)  #delete expect,express\n",
    "    f_true.replace('1353103','g',inplace=True)\n",
    "    f_true.replace('1892101','h',inplace=True)\n",
    "    f_true.replace('1892103','h',inplace=True)\n",
    "    f_true.replace('1892105','h',inplace=True)\n",
    "    f_true.replace('1892102','i',inplace=True)\n",
    "    f_true.replace('1892104','i',inplace=True)\n",
    "    f_true.replace('1892106','i',inplace=True)\n",
    "    f_true.replace('1892107','i',inplace=True)\n",
    "    f_true.replace('2439901','j',inplace=True)\n",
    "    f_true.replace('2439902','j',inplace=True)\n",
    "    f_true.replace('2439904','j',inplace=True)\n",
    "    f_true.replace('2439905','j',inplace=True)\n",
    "    f_true.replace('2439903','k',inplace=True)\n",
    "    f_true.replace('2439908','k',inplace=True)\n",
    "    f_true.replace('2439906','k',inplace=True)\n",
    "    f_true.replace('2555501','l',inplace=True)\n",
    "    f_true.replace('2555502','l',inplace=True)\n",
    "    f_true.replace('2555507','l',inplace=True)\n",
    "    f_true.replace('2555504','m',inplace=True)\n",
    "    f_true.replace('2555505','m',inplace=True)\n",
    "    f_true.replace('2555506','m',inplace=True)\n",
    "    f_true.replace('2644301','n',inplace=True)\n",
    "    f_true.replace('2644307','n',inplace=True)\n",
    "    f_true.replace('2644302','n',inplace=True)\n",
    "    f_true.replace('2644303','n',inplace=True)   # delete note\n",
    "    f_true.replace('3165218','o',inplace=True)\n",
    "    f_true.replace('3165217','o',inplace=True)\n",
    "    f_true.replace('3165213','o',inplace=True)\n",
    "    f_true.replace('3165220','p',inplace=True)\n",
    "    f_true.replace('3165214','p',inplace=True)\n",
    "    f_true.replace('3288301','q',inplace=True)\n",
    "    f_true.replace('3288306','q',inplace=True)\n",
    "    f_true.replace('3288302','r',inplace=True)\n",
    "    f_true.replace('3288305','r',inplace=True)\n",
    "    f_true.replace('3313901','s',inplace=True)\n",
    "    f_true.replace('3313906','s',inplace=True)\n",
    "    f_true.replace('3313902','t',inplace=True)\n",
    "    f_true.replace('3313905','t',inplace=True)\n",
    "    f_true.replace('3434801','u',inplace=True)\n",
    "    f_true.replace('3434806','u',inplace=True)\n",
    "    f_true.replace('3434803','v',inplace=True)\n",
    "    f_true.replace('3434807','v',inplace=True)\n",
    "    f_true.replace('3477801','w',inplace=True)\n",
    "    f_true.replace('3477803','w',inplace=True)\n",
    "    f_true.replace('3597906','x',inplace=True)\n",
    "    f_true.replace('3597907','x',inplace=True)\n",
    "    f_true.replace('3597911','x',inplace=True)\n",
    "    f_true.replace('3893501','y',inplace=True)\n",
    "    f_true.replace('3893505','y',inplace=True)\n",
    "    f_true.replace('3893507','y',inplace=True)\n",
    "    f_true.replace('3893508','y',inplace=True)\n",
    "    f_true.replace('3893502','z',inplace=True)\n",
    "    f_true.replace('3893503','z',inplace=True)\n",
    "    f_true.replace('3893509','z',inplace=True)\n",
    "    f_true.replace('4155302','aa',inplace=True)\n",
    "    f_true.replace('4155303','aa',inplace=True)\n",
    "    f_true.replace('4155304','aa',inplace=True)\n",
    "    f_true.replace('4155307','aa',inplace=True)\n",
    "    f_true.replace('4198501','bb',inplace=True)\n",
    "    f_true.replace('4198502','bb',inplace=True)\n",
    "    f_true.replace('4198503','bb',inplace=True)\n",
    "    f_true.replace('4198506','bb',inplace=True)\n",
    "    f_true.replace('4198507','bb',inplace=True)  \n",
    "    f_true.replace('4380101','cc',inplace=True)\n",
    "    f_true.replace('4380102','cc',inplace=True)\n",
    "    f_true.replace('4380103','cc',inplace=True)\n",
    "    f_true.replace('4380104','dd',inplace=True)\n",
    "    f_true.replace('4380105','dd',inplace=True)\n",
    "\n",
    "    #--------------------------------------------\n",
    "    f_true.replace('4380106','ee',inplace=True)\n",
    "    f_true.replace('4380109','ee',inplace=True)\n",
    "    f_true.replace('4530702','ff',inplace=True)\n",
    "    f_true.replace('4530704','ff',inplace=True)\n",
    "\n",
    "    f_true.replace('4636102','gg',inplace=True) \n",
    "    f_true.replace('4636101','gg',inplace=True) \n",
    "    f_true.replace('4636107','gg',inplace=True) \n",
    "    f_true.replace('4636108','gg',inplace=True) \n",
    "    f_true.replace('4636103','hh',inplace=True) \n",
    "    f_true.replace('4636104','hh',inplace=True) \n",
    "    f_true.replace('4636110','hh',inplace=True) \n",
    "    f_true.replace('4636111','hh',inplace=True) \n",
    "    f_true.replace('4636112','hh',inplace=True) \n",
    "    #---------------\n",
    "\n",
    "    f_true.replace('4640501','ii',inplace=True) \n",
    "    f_true.replace('4640502','ii',inplace=True) \n",
    "    f_true.replace('4640507','ii',inplace=True)\n",
    "    f_true.replace('4640503','jj',inplace=True) \n",
    "    f_true.replace('4640508','jj',inplace=True) \n",
    "    f_true.replace('4640509','jj',inplace=True) \n",
    "    #------------------\n",
    "    f_true.replace('4711401','kk',inplace=True) \n",
    "    f_true.replace('4711403','kk',inplace=True) \n",
    "    f_true.replace('4711405','kk',inplace=True)\n",
    "\n",
    "    f_true.replace('4711404','ll',inplace=True) \n",
    "    f_true.replace('4711406','ll',inplace=True) \n",
    "\n",
    "    #---------------------------------\n",
    "    f_true.replace('4753401','mm',inplace=True) \n",
    "    f_true.replace('4753404','mm',inplace=True) \n",
    "    f_true.replace('4753406','mm',inplace=True)\n",
    "    f_true.replace('4753403','nn',inplace=True) \n",
    "    f_true.replace('4753407','nn',inplace=True)\n",
    "    f_true.replace('4753405','oo',inplace=True) \n",
    "    f_true.replace('4753408','oo',inplace=True)\n",
    "\n",
    "\n",
    "    #ADJ  合并后意思 2.75， 141个例子， 4个单词\n",
    "\n",
    "    f_adj_answer.replace('hot%3:00:01::','a',inplace=True)\n",
    "    f_adj_answer.replace('hot%3:00:02::','a',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:active:01','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:charged:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:fast:01','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:fresh:01','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:good:01','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:illegal:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:lucky:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:near:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:new:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:popular:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:pungent:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:radioactive:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:sexy:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:skilled:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:unpleasant:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:violent:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:wanted:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:warm:03','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:02:fast:01','b',inplace=True)\n",
    "    f_adj_answer.replace('important%3:00:00::','c',inplace=True)\n",
    "    f_adj_answer.replace('important%3:00:02::','c',inplace=True)\n",
    "    f_adj_answer.replace('important%3:00:04::','c',inplace=True)\n",
    "    f_adj_answer.replace('important%5:00:00:immodest:02','d',inplace=True)\n",
    "    f_adj_answer.replace('important%5:00:00:influential:00','d',inplace=True)\n",
    "    f_adj_answer.replace('solid%3:00:01::','e',inplace=True)\n",
    "    f_adj_answer.replace('solid%3:00:02::','e',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:homogeneous:00','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:honorable:00','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:opaque:00','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:plain:02','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:sound:01','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:unbroken:02','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:undiversified:00','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:wholesome:00','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:hard:01','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:good:01','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:frozen:00','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:cubic:00','f',inplace=True)\n",
    "\n",
    "    #\n",
    "    #answer\n",
    "    #-----------------\n",
    "\n",
    "    f_adj_true.replace('hot%3:00:01::','a',inplace=True)\n",
    "    f_adj_true.replace('hot%3:00:02::','a',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:active:01','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:charged:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:fast:01','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:fresh:01','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:good:01','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:illegal:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:lucky:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:near:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:new:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:popular:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:pungent:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:radioactive:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:sexy:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:skilled:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:unpleasant:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:violent:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:wanted:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:warm:03','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:02:fast:01','b',inplace=True)\n",
    "    f_adj_true.replace('important%3:00:00::','c',inplace=True)\n",
    "    f_adj_true.replace('important%3:00:02::','c',inplace=True)\n",
    "    f_adj_true.replace('important%3:00:04::','c',inplace=True)\n",
    "    f_adj_true.replace('important%5:00:00:immodest:02','d',inplace=True)\n",
    "    f_adj_true.replace('important%5:00:00:influential:00','d',inplace=True)\n",
    "    f_adj_true.replace('solid%3:00:01::','e',inplace=True)\n",
    "    f_adj_true.replace('solid%3:00:02::','e',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:homogeneous:00','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:honorable:00','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:opaque:00','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:plain:02','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:sound:01','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:unbroken:02','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:undiversified:00','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:wholesome:00','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:hard:01','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:good:01','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:frozen:00','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:cubic:00','f',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
