{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home_lab_local/s1810410/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home_lab_local/s1810410/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home_lab_local/s1810410/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home_lab_local/s1810410/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home_lab_local/s1810410/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from gensim import matutils\n",
    "warnings.filterwarnings('ignore')\n",
    "from gensim.models import KeyedVectors\n",
    "from urllib import request\n",
    "import string\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem.porter import *\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer  \n",
    "from nltk import pos_tag\n",
    "porter_stemmer = PorterStemmer() \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "punc = '[,.!\\')(-;?:''\"``\"\"]'\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "#word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193228\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "import json, pickle\n",
    "def load_dict(file):\n",
    "    f = open(file, 'r')\n",
    "    js =f.read()\n",
    "    DIC = json.loads(js)\n",
    "    f.close()\n",
    "    return DIC\n",
    "\n",
    "# 获取字典信息\n",
    "f1 = open('corpus/GlossDict.txt', 'r') \n",
    "f2 = open('corpus/GlossDictExpand.txt', 'r')\n",
    "dic = eval(f1.read())   # str into dict\n",
    "dic2 = eval(f2.read())\n",
    "glossDict = dic\n",
    "ExpandglossDict = dic2\n",
    "f1.close()\n",
    "f2.close()\n",
    "\n",
    "# 获取文章信息和文章ID\n",
    "f3 = open('corpus/ID.txt', 'r')\n",
    "f4 = open('corpus/INSTANCE.txt', 'r')\n",
    "id_list = [] # 3944\n",
    "article = [] # 3944\n",
    "for line in f3.readlines():\n",
    "    line = line.strip('\\n')\n",
    "    line = line.strip('\"')\n",
    "    id_list.append(line)\n",
    "\n",
    "for art in f4.readlines():\n",
    "    art = art.strip('\\n')\n",
    "    art = art.strip('[,],\\\\n,\",')\n",
    "    article.append(art)\n",
    "f3.close()\n",
    "f4.close()\n",
    "f6 = open('corpus/GlossDictWITHpunc.txt', 'r') \n",
    "dic6 = eval(f6.read())   # str into dict\n",
    "glossWithPunc = list(dic6)\n",
    "f6.close()\n",
    "EPGwithSTOP = open('corpus/EPGwithSTOP.txt', 'r') \n",
    "dicEPG = eval(EPGwithSTOP.read())  \n",
    "EPGwith = list(dicEPG)                                 # wrod 保留stopwords； 额外信息没有stopwords\n",
    "EPGwithSTOP.close()\n",
    "# ----------corpus------------\n",
    "with open(r\"corpus/corp.txt\", \"r\") as fo:\n",
    "    out = fo.read()\n",
    "    sentence_corpus = json.loads(out)\n",
    "\n",
    "#with open(r'new-corpus/new-corpus-list-news-2010-1M.txt', 'rb') as fp:\n",
    "#    sentence_corpus2 = pickle.load(fp)\n",
    "\n",
    "#with open(r'new-corpus/new-corpus-wiki-2016-1M.txt', 'rb') as fp:\n",
    "#    sentence_corpus3 = pickle.load(fp)\n",
    "# dict\n",
    "glossAND_stop = load_dict('./corpus/glossAND_stop_dict.txt') # 包含stopwords的词典信息\n",
    "glossDict = dic                                              # 不包含stopwrods的词典信息\n",
    "hypANDstop = load_dict('./corpus/hypANDstop_dict.txt')    # 加入额外信息的包含stopwords的词典信息\n",
    "hypAND = load_dict('./corpus/hypAND_dict.txt')            # 加入额外信息的不包含stopwords的词典信息\n",
    "EPGwith = list(dicEPG)        \n",
    "print(len(sentence_corpus))\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there',\n",
       " 'be',\n",
       " 'no',\n",
       " 'trace',\n",
       " 'leave',\n",
       " 'on',\n",
       " 'it',\n",
       " 'only',\n",
       " 'the',\n",
       " 'wood',\n",
       " 'be',\n",
       " 'still',\n",
       " 'damp']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_corpus[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence_corpus = sentence_corpus2 + sentence_corpus1 + sentence_corpus3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer 词性还原\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    res = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word, pos in pos_tag(word_tokenize(sentence)):\n",
    "        wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "        res.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "    return res\n",
    "\n",
    "article = list(map(lambda x: ' '.join(lemmatize_sentence(x)), article))\n",
    "\n",
    "# clean data with oov, punc, lower characters\n",
    "def clean(word_list):\n",
    "    new_sent = word_list\n",
    "    new_sent = [word for word in new_sent if word in word_vectors.wv.vocab]\n",
    "    new_sent = [word for word in new_sent if word not in punc]\n",
    "    new_sent = [w.lower() for w in new_sent]\n",
    "   #new_sent = [word for word in new_sent if word not in stop_words]\n",
    "    return new_sent\n",
    "\n",
    "# delete short sentences in corpus, lemmatize\n",
    "def txt2wordlist(text):\n",
    "    afterClean = []\n",
    "    text = sent_tokenize(text)\n",
    "    for i in text:\n",
    "        i = lemmatize_sentence(i)\n",
    "        i = clean(i)\n",
    "        if len(i) > 5:\n",
    "            afterClean.append(i)\n",
    "    return afterClean\n",
    "\n",
    "# find collocations for a target word in test data and windowsize words\n",
    "def Find_collo(keyword,s,e,n):\n",
    "    collocation = []\n",
    "    KeyWord_art = []\n",
    "    target = article[s:e]\n",
    "    for art in target:\n",
    "        new_sent = lemmatize_sentence(art)\n",
    "        new_sent = [word for word in new_sent if word not in punc]\n",
    "        new_sent = [word.lower() for word in new_sent]\n",
    "        with_stop_sent = new_sent\n",
    "        new_sent = clean(new_sent)\n",
    "\n",
    "        key_index2 = [j for j,x in enumerate(with_stop_sent) if x.find(keyword)!=-1][0]\n",
    "        key_index =[j for j,x in enumerate(new_sent) if x.find(keyword)!=-1][0]\n",
    "\n",
    "        key_art = new_sent[(key_index-n):key_index] + new_sent[(key_index):(key_index+n+1)]\n",
    "        key_collo = with_stop_sent[key_index2-5:key_index2+6] \n",
    "\n",
    "        #key_collo2 = new_sent[key_index-1] + ' ' new_sent[key_index]\n",
    "        KeyWord_art.append(key_art)\n",
    "        collocation.append(key_collo)\n",
    "\n",
    "\n",
    "    return collocation, KeyWord_art\n",
    "\n",
    "# extract surrounding texts for a word, n is window size\n",
    "def surround_context(keyword,corpus,n):\n",
    "    instance_list = []\n",
    "    for sent in sentence_corpus:\n",
    "       #key_index =[j for j,x in enumerate(sentence) if x.find(keyword)!=-1] #Fuzzy lookup\n",
    "        key_index =[j for j,x in enumerate(sent) if x == keyword]\n",
    "        if len(key_index) != 0:\n",
    "            i = key_index[0]   \n",
    "            new_list = sent[(i-n):i] + sent[(i):(i+n+1)]\n",
    "            instance_list.append(new_list)         \n",
    "    return instance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "noun_key_value = re.compile(r'[a-z]+[%][\\d]+[:][\\d]+[:][\\d]+[:]+[a-z]*[:]*[\\d\\d]*')\n",
    "def RE(S):\n",
    "    tag = re.findall('\\d\\d+', S)[0]  # extraction number\n",
    "    collo = re.sub(' [0-9]+','',S)  #  sub number\n",
    "    return tag, collo\n",
    "\n",
    "def RE2(S):\n",
    "    tag = re.search(noun_key_value, S).group()\n",
    "    collo = re.sub(noun_key_value,'',S)\n",
    "    return tag, collo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为目标单词在corpus中找短语, \n",
    "# param: TEXT-> corpus sentecne list, Word-> target word, \n",
    "# a,b-> start end index in gloss dict, n-> n pairs for similarity compute\n",
    "# T -> threshold score of similarity\n",
    "\n",
    "def tag_collo(TEXT,Word,a,b,n,T):\n",
    "    score_list = []\n",
    "    answer_key_list = []\n",
    "    Score = []\n",
    "    L_colo = []\n",
    "    i=0\n",
    "    while i in range(len(TEXT)):\n",
    "        sim_score = []\n",
    "        for d in EPGwith[a:b]:\n",
    "            #sim = word_vectors.n_similarity(d['gloss'],TEXT[i])   #修改过\n",
    "            sim = SelectWordsSim(d['gloss'], TEXT[i], n) # ith gloss, ith text, nwords\n",
    "            sim_score.append(sim)\n",
    "        max_index = sim_score.index(max(sim_score))\n",
    "        max_score_key = EPGwith[a:b][max_index]['key']  # change dict\n",
    "        gloss_words = EPGwith[a:b][max_index]['gloss']  # change dict\n",
    "        max_score = max(sim_score)\n",
    "        dict={}.fromkeys(('collocation','tag','gloss')) \n",
    "        word_index = [j for j,x in enumerate(TEXT[i]) if x.find(Word)!=-1]\n",
    "        if len(word_index)>0: \n",
    "            word_index = word_index[0]\n",
    "            try:\n",
    "                COLLO = TEXT[i][word_index-2:word_index+3]\n",
    "                if max_score > T and len(COLLO)!=0:\n",
    "                    score_list.append(max_score)\n",
    "                    answer_key_list.append(str(max_score_key))\n",
    "                    Score.append(max_score) \n",
    "                    dict['collocation'] = COLLO\n",
    "                    dict['tag'] = max_score_key\n",
    "                    dict['gloss'] = gloss_words #\n",
    "                    L_colo.append(dict)\n",
    "                i = i+1\n",
    "            except Exception as e:\n",
    "                i = i+1\n",
    "        else:\n",
    "            i = i+1\n",
    "    return L_colo,Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tag_collo_with_filter(word, corpus, n, gs, ge, T, PATTERN):\n",
    "    SC = surround_context(word, corpus, n)\n",
    "    SC_LEM = []\n",
    "    for i in SC:\n",
    "        SC_txt = ' '.join(i)\n",
    "        SC_lem = lemmatize_sentence(SC_txt)\n",
    "        SC_lem = clean(SC_lem)\n",
    "        SC_LEM.append(SC_lem)\n",
    "        \n",
    "    collocations = tag_collo(SC_LEM, word, gs, ge, 8, T)[0]\n",
    "    c_ = []\n",
    "    for i in collocations:\n",
    "        c_tag_d = {}\n",
    "        a = i['collocation']\n",
    "        a_tag = i['tag']\n",
    "        if len(a) == 5:\n",
    "            try:\n",
    "                a.index(word)\n",
    "            except:\n",
    "                continue\n",
    "            else:\n",
    "                c1, c2, c3, c4, c5 = a[:3], a[1:3], a[1:4], a[2:4], a[2:]\n",
    "                if len(c1)>=2: c_tag_d[' '.join(c5)] = a_tag\n",
    "                if len(c2)>=2: c_tag_d[' '.join(c4)] = a_tag\n",
    "                if len(c3)>=2: c_tag_d[' '.join(c3)] = a_tag\n",
    "                if len(c4)>=2: c_tag_d[' '.join(c2)] = a_tag\n",
    "                if len(c5)>=2: c_tag_d[' '.join(c1)] = a_tag\n",
    "            c_.append(c_tag_d)\n",
    "            \n",
    "    list1 = []     # list1 is collocations without tag\n",
    "    list2 = []     # list2 is collocations with tag\n",
    "    for i in range(len(c_)):\n",
    "        for k,v in c_[i].items():\n",
    "            c = k+' '+v\n",
    "            list1.append(k)\n",
    "            list2.append(c)\n",
    "    a=Counter(list1)\n",
    "    b=Counter(list2)\n",
    "    left = []    # (短语， 总共出现次数， 短语+意思， 该意思出现次数)\n",
    "    for k,v in a.items():\n",
    "        for k1,v1 in b.items():\n",
    "            k_sub = PATTERN(k1)[1]\n",
    "          #  if k == k_sub:       # verb\n",
    "            if k+' '==k_sub:   # noun or adj\n",
    "                left.append((k,v,k1,v1))\n",
    "        \n",
    "    def takeSecond(elem): # Sort by second element\n",
    "        return elem[1]  \n",
    "    left.sort(key=takeSecond, reverse=True)\n",
    "    \n",
    "    #LEFT = []\n",
    "    #for i in left:\n",
    "    #  if i[1]>=5 and i[3]/i[1] >= 0.6:\n",
    "    #    LEFT.append(i[2])\n",
    "\n",
    "    # new rate compute method = p(s|c)/p(s)\n",
    "    key_list = []\n",
    "    for ele in left:\n",
    "        key = PATTERN(ele[2])[0]\n",
    "        key_list.append(key)\n",
    "    cont = Counter(key_list) # get all sense and make a dict\n",
    "    \n",
    "    LEFT=[]\n",
    "    scores = []\n",
    "    for ele in left:\n",
    "        p_s_c = ele[3]/ele[1]\n",
    "        #key = PATTERN(ele[2])[0]\n",
    "        #sense_count = cont[key]\n",
    "        #p_s = sense_count/sum(cont.values())\n",
    "        rate =  p_s_c\n",
    "        if rate > 0.4 and ele[3]>=6:    # (0.5 2 noun) (0.7 4 adj) (0.7, 3 verb)\n",
    "            LEFT.append(ele[2]) \n",
    "            scores.append((ele[2], rate)) \n",
    "    DICT = {}\n",
    "    for i,j in map(PATTERN, LEFT):    #v-> RE, n-> RE2\n",
    "        DICT[j] = i\n",
    "\n",
    "    verb_delted_list = [word+' what','n\\'t '+word, word+' but', 'but '+word, word+' or','or '+word,'the '+word, word+' the', 'in '+word, 'a '+word, word+' a', word+' that', 'that '+word, word+' it', 'it '+word]\n",
    "    noun_delted_list = ['an '+word+' ', word+' would ', 'be '+word+' ', word+' he', word+' this ', 'this '+word+' ', word+ ' be ', 'or '+word+' ', word+ ' i ', 'any '+word+' ', 'the '+word+' the ', 'it '+word+' ',word+ ' his ',word+' or ', word+' but ',word+' that ', 'the '+word+' ', word+' the ', 'in '+word+' ', word+' in ', 'at '+'word'+' ', 'for '+word+' ', word+' at ', word+' for ', 'it '+word+' ', word+' it ', 'a '+word+' ', word+' a', word+' or'] \n",
    "    \n",
    "    for i in noun_delted_list:\n",
    "        try:\n",
    "            del DICT[i]\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    return DICT, scores, LEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an target_word \n",
      "target_word would \n",
      "be target_word \n",
      "target_word he\n",
      "target_word this \n",
      "this target_word \n",
      "target_word be \n",
      "or target_word \n",
      "target_word i \n",
      "any target_word \n",
      "the target_word the \n",
      "it target_word \n",
      "target_word his \n",
      "target_word or \n",
      "target_word but \n",
      "target_word that \n",
      "the target_word \n",
      "target_word the \n",
      "in target_word \n",
      "target_word in \n",
      "at word \n",
      "for target_word \n",
      "target_word at \n",
      "target_word for \n",
      "it target_word \n",
      "target_word it \n",
      "a target_word \n",
      "target_word a\n",
      "target_word or\n"
     ]
    }
   ],
   "source": [
    "word = 'target_word'\n",
    "noun_delted_list = ['an '+word+' ', word+' would ', 'be '+word+' ', word+' he', word+' this ', 'this '+word+' ', word+ ' be ', 'or '+word+' ', word+ ' i ', 'any '+word+' ', 'the '+word+' the ', 'it '+word+' ',word+ ' his ',word+' or ', word+' but ',word+' that ', 'the '+word+' ', word+' the ', 'in '+word+' ', word+' in ', 'at '+'word'+' ', 'for '+word+' ', word+' at ', word+' for ', 'it '+word+' ', word+' it ', 'a '+word+' ', word+' a', word+' or']\n",
    "for i in noun_delted_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算相似度（topn pairs方法）\n",
    "import heapq\n",
    "def SelectWordsSim(Glo,Cont,n):\n",
    "    total = []\n",
    "    cont_word = []\n",
    "    Glo = [word for word in Glo if word in word_vectors.vocab]\n",
    "    Cont = [word for word in Cont if word in word_vectors.vocab]\n",
    "    for word1 in Glo:\n",
    "        for word2 in Cont:\n",
    "            sim = word_vectors.similarity(word1,word2)\n",
    "            cont_word.append(word2)\n",
    "            total.append(sim)\n",
    "    topn = heapq.nlargest(n,total)   # top n words\n",
    "    new_words = []\n",
    "    for i in topn:\n",
    "        index = total.index(i)\n",
    "        selected_word = cont_word[index]\n",
    "        new_words.append(selected_word)\n",
    "    new_score = word_vectors.n_similarity(Glo,new_words)\n",
    "    return new_score\n",
    "\n",
    "# if no collocation has been matched, tag sense-key by computing\n",
    "def Tag_origi(article,gloss):\n",
    "    scores = []\n",
    "    for i in range(len(gloss)):\n",
    "        glo = gloss[i]['gloss']\n",
    "        sim = compute_sim_bert(glo, article)   # <- bert\n",
    "        #sim = SelectWordsSim(glo, article, 8)  # <- 表现最好  HRWE\n",
    "        #glo = [word for word in glo if word in word_vectors.vocab]\n",
    "        #article = [word for word in article if word in word_vectors.vocab]\n",
    "        #sim = word_vectors.n_similarity(glo,article)  # <- base\n",
    "        scores.append(sim)\n",
    "        index = scores.index(max(scores))\n",
    "        tag = gloss[index]['key']\n",
    "\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6948754"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bert-embedding tes\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "import math \n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
    "tf.config.experimental.set_visible_devices(devices=gpus[1:],device_type='GPU') \n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def compute_sim_bert(glo, article):\n",
    "    #取bert最后一层的输出, 做平均, 计算向量余弦相似度\n",
    "    glo = ' '.join(glo)\n",
    "    article = ' '.join(article)\n",
    "    glo_id = tokenizer.encode(glo, return_tensors=\"tf\")\n",
    "    article_id = tokenizer.encode(article, return_tensors=\"tf\")\n",
    "    \n",
    "    glo_out, article_out = model(glo_id), model(article_id)\n",
    "    glo_vec, article_vec =  tf.squeeze(glo_out[0]), tf.squeeze(article_out[0])   # 减去数值为1的维度\n",
    "    \n",
    "    glo_avg, article_avg= tf.reduce_mean(glo_vec, 0), tf.reduce_mean(article_vec, 0)   # 计算向量均值\n",
    "    score = cosine_similarity([glo_avg, article_avg])[0][1]\n",
    "    return score\n",
    "    \n",
    "a = ['i', 'have', 'an', 'apple']\n",
    "b  = ['i', 'like', 'you']\n",
    "compute_sim_bert(a,b)\n",
    "# bert--over-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all collo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最终整合函数！！！！\n",
    "def Compute_(Keyword, Collocations, Articles, GS, GE):\n",
    "    target_texts=[]\n",
    "    Colla=[]\n",
    "    DataColla=[]\n",
    "    DATA=()\n",
    "    DataArt=[]\n",
    "    Colla = Collocations \n",
    "    DataColla = Articles[0]  # collo in test data for target word\n",
    "    DataArt = Articles[1] \n",
    "\n",
    "    AN = []   # answer key\n",
    "    colloAN =[]  # answer tagged by collo\n",
    "    record_id = []\n",
    "    _COLLO = []\n",
    "    for i,text in enumerate(DataColla):\n",
    "    # inner loop should be interrupted when a collo is meet the same one \n",
    "    # and skip this time for outer loop; 这个循环检查过没有问题\n",
    "        flag = False\n",
    "        t1 = ' '.join(text[3:6])\n",
    "        t2 = ' '.join(text[4:6])\n",
    "        t3 = ' '.join(text[4:7])\n",
    "        t4 = ' '.join(text[5:7])\n",
    "        t5 = ' '.join(text[5:8])\n",
    "        t_all = ' '.join(text[3:8])\n",
    "        \n",
    "        for k,v in Colla.items():\n",
    "          #if k in t_all:\n",
    "            if t1+' '==k or t2+' '==k or t3+' '==k or t4+' '==k or t5+' '==k:  # noun & adj\n",
    "            #if t1==k or t2==k or t3==k or t4==k or t5==k:                      # verb\n",
    "                AN.append(v)\n",
    "                colloAN.append(v)\n",
    "                record_id.append(i)\n",
    "                _COLLO.append(k)\n",
    "                flag = True\n",
    "                break\n",
    "        if flag == True:\n",
    "            continue\n",
    "        else:\n",
    "            Tag = Tag_origi(DataArt[i], glossDict[GS:GE])#glossDict[GS:GE])  ## change dict!!  between verb and noun\n",
    "            AN.append(Tag)\n",
    "    return AN, colloAN, record_id, _COLLO  #修改过"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BATCH_compute(keyword, GS, GE, T, AS, AE):\n",
    "    collocations, articles, answer = None, None, None \n",
    "    collocations = Tag_collo_with_filter(keyword, sentence_corpus, 8, GS,GE,T, RE2)[0]  # RE -> verb, RE2 -> noun/adj\n",
    "    articles = Find_collo(keyword, AS, AE, 10)       # 10 -> noun,adj  9 -> verb\n",
    "    answer = Compute_(keyword, collocations, articles, GS, GE)\n",
    "    return answer[0], answer[1], answer[2], answer[3]\n",
    "\n",
    "def BATCH_count(keyword, GS, GE, T, AS, AE):\n",
    "    collocations, articles, answer = None, None, None \n",
    "    collocations = Tag_collo_with_filter(keyword, sentence_corpus, 8, GS,GE,T, RE)  # RE -> verb, RE2 -> noun/adj\n",
    "    return collocations[0]#len(collocations[0]), collocations[3]\n",
    "\n",
    "# number \n",
    "#verb: 87125 -> 229\n",
    "# noun: 25410 -> 104\n",
    "# adj: 7635 -> 165\n",
    "# all : 120170  -> 498"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verb word colo num : 87125 ; filtered: 229\n",
    "# noun word colo num : 25410; filtered: 136\n",
    "# adj   7635  14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in LS_v:\n",
    "    a = BATCH_count(i[2], i[3], i[4], 0.75, i[0], i[1])\n",
    "    res.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting...\n",
      "different --> runned over!\n",
      "hot --> runned over!\n",
      "important --> runned over!\n",
      "solid --> runned over!\n",
      "Exe time---: 16.819820700000005 min\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.clock()\n",
    "answer_file = []\n",
    "answer_file_tag_with_collo = []\n",
    "id_tag_with_collo = []\n",
    "matched_collo = []\n",
    "\n",
    "print('starting...')\n",
    "for i in LS_adj:\n",
    "    a = BATCH_compute(i[2], i[3], i[4], 1.0, i[0], i[1])  # 1.0 不要coll; (0.7 noun,adj)(0.75 verb)\n",
    "    matched_collo.append(a[3])\n",
    "    answer_file.append(a[0])\n",
    "    answer_file_tag_with_collo.append(a[1])\n",
    "    id_tag_with_collo.append(a[2])\n",
    "    print(\"%s --> runned over!\" % (str(i[2])))     \n",
    "end = time.clock()\n",
    "print('Exe time---:', (end-start)/60,'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_file = 'corpus/verb-bert.txt'  # corpus/verb.txt\n",
    "noun_file = 'corpus/noun-bert.txt'\n",
    "adj_file = 'corpus/adj-bert.txt'\n",
    "with open(adj_file,'w') as f:  # change\n",
    "    for answer in answer_file:\n",
    "        for key in answer:\n",
    "            f.write(key)\n",
    "            f.write('\\n')\n",
    "\n",
    "f_true = pd.read_csv('corpus/v_true.txt',sep=' ',names=['0','1','2','3','4'], header=None, engine='python')\n",
    "f_noun_true = pd.read_csv('corpus/N_true.txt',sep=' ',names=['0','1','2','3','4'], header=None ,engine='python')\n",
    "f_adj_true = pd.read_csv('corpus/all_adj_answer.txt',sep=' ',names=['0','1','2','3','4'],header=None,engine='python')\n",
    "\n",
    "f_answer = pd.read_csv(verb_file, sep=' ', header=None, dtype={0:str})  #修改第0列数据类型为str\n",
    "f_noun_answer = pd.read_csv(noun_file,sep =' ',header=None)\n",
    "f_adj_answer = pd.read_csv(adj_file,sep = ' ',header=None)\n",
    "sense_merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " collo acc: 0 \n",
      " all acc: 0.5602836879432624\n"
     ]
    }
   ],
   "source": [
    "VERB_id = []\n",
    "NOUN_id = []\n",
    "ADJ_id = []\n",
    "j = 0\n",
    "for i in LS_adj:    # LS_n\n",
    "    id = i[1] - i[0]\n",
    "    a = [j, id+j]\n",
    "    ADJ_id.append(a)# NOUN_id\n",
    "    j = id+j\n",
    "\n",
    "#print(' collo acc:',compute_acc(NOUN_id, f_noun_true, f_noun_answer),'\\n',\n",
    "#      'all acc:',newcompare(f_noun_true, f_noun_answer)/1570)    # 1698 / 1570\n",
    "\n",
    "print(' collo acc:',compute_acc(ADJ_id, f_adj_true, f_adj_answer),'\\n',\n",
    "      'all acc:',newcompare(f_adj_true, f_adj_answer)/141)    # 1698 / 1570\n",
    "\n",
    "#print(' collo acc:',compute_acc(VERB_id, f_true, f_answer),'\\n',\n",
    "#      'all acc:',newcompare(f_true, f_answer)/1698)    # 1698 / 1570\n",
    "\n",
    "#verb_acc.append(newcompare(f_true, f_answer))\n",
    "\n",
    "#NOUN\n",
    "# ws hrwe(5, 0.505)(8, 0.521)(10, 0.534)\n",
    "# ws base (5, 0.513, 8,0.512,  10,0.506)\n",
    "# Noun collocation 0.631\n",
    "# expandas dic info 0.505 ; \n",
    "\n",
    "#verb:\n",
    "# hrwe window-size 5: 0.563  8: 0.572 9: 0.583  10: 0.572\n",
    "# base  ws 5: 0.567   8: 0.548  9: 0.541   10: 0.533\n",
    "\n",
    "# -----------exp gloss result(base)----------\n",
    "# exp -> 0.517 (verb)\n",
    "# exp -> 0.447 (adj)\n",
    "#---------bert result----------\n",
    "# verb  0.431,  \n",
    "# noun   0.494  , glove 0.462, \n",
    "# adj 0.518  ,   glove 0.496\n",
    "#----------adj window----\n",
    "# 5 -> 0.567, 8 -> 0.539 , 10 -> 0.560  (base)\n",
    "# 5 -> 0.525, 8 -> 0.504 , 10 -> 0.511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#           (37->0.919)  0.540  6\n",
    "#           (46->0.869)   0.539   5\n",
    "#          (67->0.806)  0.541   4\n",
    "# news-100 (86->0.67)  0.541    3  \n",
    "#         (115->0.652) 0.538    2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big corpus \n",
    "# noun- col(117->0.628, all-acc: 0.536)    121 -> 0.653\n",
    "# verb- col(340 -> 0.541, all-acc: 0.585, 0.75)  ;2949\n",
    "\n",
    "# small corpus\n",
    "# verb- col (221-> 0.548, all-acc: 0.587, 0.75) ; 373\n",
    "# noun- col (68 -> 0.735, all-acc:0.545 , 0.7) ; 68\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun-final\n",
    "# ws 10 \n",
    "#(base 0.506, \n",
    "# base+word collo 0.516   (T:0.7 word collo 0.5, 2)\n",
    "# base+ depend collo 0.510 (depend collo 5 0.7)\n",
    "# hrwe 0.534    \n",
    "# hrwe + word collo 0.545\n",
    "# hrwe + depd collo  0.540\n",
    "# hrwe + word collo + hrwe collo  )\n",
    "# collo setting: rate: 0.5, appera: 2 T: 0.7\n",
    "\n",
    "#  collo only (word collo 0.63->279)\n",
    "\n",
    "\n",
    "# verb - finale\n",
    "# ws 9 (base, base+collo , hrwe , hrwe+collo ,)\n",
    "# hrwe + wordcollo 0.587\n",
    "# collo setting: rate 0.7, appear 3, T 0.75\n",
    "# collo only 0.573 -> 618 ()\n",
    "# ensemble 0.594\n",
    "\n",
    "# adj-final\n",
    "# ws 10, word collo: 0.7, 4  \n",
    "#(base 0.560, \n",
    "# base+word collo 0.553 (T: 0.7   collo(num, acc)-> 6, 0.33), \n",
    "# base + depd collo 0.546( )\n",
    "# hrwe 0.511 \n",
    "# hrwe+ word collo 0.511 (T: 0.7 collo(num,acc)-> 6, 0.33)\n",
    "# hrwe + depend collo 0.511 ( depend collo setting:5 0.7, depd-collo(num,acc)-> )\n",
    "# hrwe + depd collo + word collo 0.511 \n",
    "# collo 0.625  -> 16\n",
    "\n",
    "\n",
    "#all (base 0.525, HRWE 0.557, hrwe+collo 0.564)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5359598122616603"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.553*1698 + 0.516* 1570 + 0.553*141) / (1698+1570+141)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AccSort(ID, TRUE, ANSWER, POS):\n",
    "    #计算单个单词的精确度，观察固定搭配的熵是否有影响\n",
    "    \n",
    "    ret = []\n",
    "    _,Colcorrect,Collen = comp_single(ID, TRUE, ANSWER)\n",
    "    for k,i in enumerate(ID):\n",
    "        word = POS[k][2]\n",
    "        l,r = i[0], i[1]\n",
    "        length = r-l\n",
    "        res = newcompare(TRUE[l:r], ANSWER[l:r])/length         #1570  # accuracy for all noun instances\n",
    "        collo_acc = Colcorrect[k] / (Collen[k]+0.001)\n",
    "        rate = Collen[k] / length\n",
    "        #ret.append([word, round(res,2), round(collo_acc,2), round(rate, 2)])\n",
    "        ret.append([word, round(res, 2)])\n",
    "    return ret\n",
    "\n",
    "T = AccSort(VERB_id, f_true, f_answer, LS_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算collo only 方法需要用\n",
    "all_id = []\n",
    "for i,e in enumerate(VERB_id):\n",
    "    if len(id_tag_with_collo[i]) != 0:\n",
    "        for n in id_tag_with_collo[i]:\n",
    "            new_n = n + e[0]\n",
    "            all_id.append(new_n)\n",
    "print(all_id, len(all_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LS_v = [(0,114,'activate',0,5),(114,246,'add',5,11),(246,379,'appear',11,14),(623,754,'ask',25,31),\n",
    "     \n",
    "        (1067,1146,'begin',51,55),(1146,1213,'climb',55,60),(1690,1777,'eat',89,96),\n",
    "      \n",
    "        (1777,1842,'encounter',96,100),\n",
    "      \n",
    "        (1975,2007,'hear',107,114),(2268,2304,'lose',162,171),(2304,2344,'mean',171,178),(2344,2374,'miss',178,186)\n",
    "     \n",
    "        ,(2919,2971,'play',221,233),(2971,3065,'produce',233,239),(3065,3134,'provide',239,245),\n",
    "     \n",
    "        (3134,3161,'receive',245,254),(3161,3231,'remain',254,257),(3231,3261,'rule',257,262),(3377,3432,'smell',273,280),\n",
    "     \n",
    "        (3589,3653,'suspend',305,312),(3653,3726,'talk',312,321),(3726,3783,'treat',321,330),(3783,3797,'use',330,335),\n",
    "     \n",
    "        (3797,3831,'wash',335,347),(3831,3882,'watch',347,354),(3882,3921,'win',354,361),(3921,3944,'write',361,369)]\n",
    "      \n",
    "\n",
    "LS_n = [(379,490,'argument',14,19),(490,623,'arm',19,25),\n",
    "         \n",
    "         (935,1067,'bank',41,51),(1275,1403,'degree',64,71),(1403,1517,'difference',71,76),(1567,1590,'difficulty',81,85),\n",
    "         \n",
    "         (1590,1690,'disc',85,89),(2050,2124,'image',136,143),(2143,2236,'interest',148,155),(2236,2268,'judgment',155,162),\n",
    "         (2515,2632,'paper',201,208),(2632,2748,'party',208,213),(2748,2835,'performance',213,218),\n",
    "         \n",
    "         (2835,2919,'plan',218,221),(3261,3359,'shelter',262,266),(3461,3557,'sort',294,298),(3557,3589,'source',298,305)]\n",
    " \n",
    "\n",
    "LS_adj = [(1517,1567,'different',76,81),(2007,2050,'hot',114,136),\n",
    "          (2124,2143,'important',143,148),(3432,3461,'solid',280,294)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(LS_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verb: 27; noun: 17;  adj: 4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "different, \n",
      "hot, \n",
      "important, \n",
      "solid, \n"
     ]
    }
   ],
   "source": [
    "for i in LS_adj:\n",
    "    print(i[2]+', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(index, tru, ans):   \n",
    "    # 计算每个单词的collo的acc\n",
    "    LEN_collo = []\n",
    "    correct_number = []\n",
    "    for i in range(len(id_tag_with_collo)):\n",
    "        INDEX = id_tag_with_collo[i]\n",
    "        A = index[i] ##\n",
    "        c = colloComapre(tru, ans, A[0], A[1], INDEX)  ##\n",
    "        LEN_collo.append(len(INDEX))\n",
    "        correct_number.append(c)\n",
    "    if sum(LEN_collo) != 0:\n",
    "        return sum(LEN_collo), sum(correct_number)/sum(LEN_collo)  # p for tagged by collo insatnces\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def newcompare(TRUE,ANSWER):\n",
    "    correct = []\n",
    "    for i in range(len(ANSWER)):\n",
    "        if ANSWER.iloc[i][0] in TRUE.iloc[i][2:5].values:\n",
    "            correct.append(1)\n",
    "    return len(correct)\n",
    "\n",
    "def colloComapre(TRUE, ANSWER, AS, AE, INDEX):\n",
    "    newTrue = None\n",
    "    newAnswer = None\n",
    "    number = None\n",
    "\n",
    "    newTrue = TRUE[AS:AE]\n",
    "    newAnswer = ANSWER[AS:AE]\n",
    "    number = []\n",
    "    if len(INDEX) == 0:\n",
    "        return 0\n",
    "    else:    \n",
    "        for index in INDEX:\n",
    "            answer = newAnswer.iloc[index][0]\n",
    "            Tr = newTrue.iloc[index][2:5].values\n",
    "            if answer in Tr:\n",
    "                number.append(1)\n",
    "    return len(number)\n",
    "\n",
    "\n",
    "def comp_single(ID, f_tr, f_an):\n",
    "    LEN_collo = []\n",
    "    correct_number = []\n",
    "    for i in range(len(id_tag_with_collo)):\n",
    "        INDEX = id_tag_with_collo[i]\n",
    "        A = ID[i] ##\n",
    "        c = colloComapre(f_tr, f_an, A[0], A[1], INDEX)  ##\n",
    "        LEN_collo.append(len(INDEX))\n",
    "        correct_number.append(c)\n",
    "    acc = []\n",
    "    for idx, i in enumerate(LEN_collo):\n",
    "        if i != 0:\n",
    "            ac = correct_number[idx]/i\n",
    "            acc.append(ac)\n",
    "        else:\n",
    "            acc.append(0)\n",
    "    return acc, correct_number, LEN_collo # CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noune  合并后平均意思3.58， 17个单词， 1570个例子\n",
    "def sense_merge():\n",
    "    f_noun_answer.replace('argument%1:10:00::','a',inplace=True)\n",
    "    f_noun_answer.replace('argument%1:10:00::','a',inplace=True)\n",
    "    f_noun_answer.replace('bank%1:06:00::','b',inplace=True)\n",
    "    f_noun_answer.replace('bank%1:06:01::','b',inplace=True)\n",
    "    f_noun_answer.replace('bank%1:14:00::','b',inplace=True)\n",
    "    f_noun_answer.replace('bank%1:14:01::','b',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('bank%1:17:00::','c',inplace=True)\n",
    "    f_noun_answer.replace('bank%1:17:01::','c',inplace=True)\n",
    "    f_noun_answer.replace('bank%1:17:02::','c',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('bank%1:21:00::','d',inplace=True)\n",
    "    f_noun_answer.replace('bank%1:21:01::','d',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('degree%1:07:00::','e',inplace=True)\n",
    "    f_noun_answer.replace('degree%1:07:01::','e',inplace=True)\n",
    "    f_noun_answer.replace('degree%1:26:01::','e',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('image%1:06:00::','f',inplace=True)\n",
    "    f_noun_answer.replace('image%1:06:01::','f',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('image%1:07:00::','g',inplace=True)\n",
    "    f_noun_answer.replace('image%1:09:00::','g',inplace=True)\n",
    "    f_noun_answer.replace('image%1:09:02::','g',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('interest%1:04:01::','h',inplace=True)\n",
    "    f_noun_answer.replace('interest%1:07:01::','h',inplace=True)\n",
    "    f_noun_answer.replace('interest%1:07:02::','h',inplace=True)\n",
    "    f_noun_answer.replace('interest%1:09:00::','h',inplace=True)\n",
    "    f_noun_answer.replace('interest%1:14:00::','h',inplace=True)\n",
    "    f_noun_answer.replace('interest%1:21:00::','i',inplace=True)\n",
    "    f_noun_answer.replace('interest%1:21:03::','i',inplace=True)\n",
    "\n",
    "    #------------------\n",
    "    f_noun_answer.replace('judgment%1:04:00::','j',inplace=True)\n",
    "    f_noun_answer.replace('judgment%1:04:02::','j',inplace=True)\n",
    "    f_noun_answer.replace('judgment%1:10:00::','j',inplace=True)\n",
    "    f_noun_answer.replace('judgment%1:07:00::','k',inplace=True)\n",
    "    f_noun_answer.replace('judgment%1:09:00::','k',inplace=True)\n",
    "    f_noun_answer.replace('judgment%1:09:01::','k',inplace=True)\n",
    "    f_noun_answer.replace('judgment%1:09:04::','k',inplace=True)\n",
    "    #----------------\n",
    "\n",
    "    f_noun_answer.replace('paper%1:10:01::','l',inplace=True)\n",
    "    f_noun_answer.replace('paper%1:10:02::','l',inplace=True)\n",
    "    f_noun_answer.replace('paper%1:06:00::','m',inplace=True)\n",
    "    f_noun_answer.replace('paper%1:10:03::','m',inplace=True)\n",
    "    f_noun_answer.replace('paper%1:14:00::','m',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('paper%1:10:00::','n',inplace=True)\n",
    "    f_noun_answer.replace('paper%1:27:00::','n',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('party%1:11:00::','o',inplace=True)\n",
    "    f_noun_answer.replace('party%1:14:02::','o',inplace=True)\n",
    "    f_noun_answer.replace('party%1:18:00::','o',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('performance%1:04:00::','p',inplace=True)\n",
    "    f_noun_answer.replace('performance%1:04:01::','p',inplace=True)\n",
    "    f_noun_answer.replace('performance%1:10:00::','p',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('performance%1:04:03::','q',inplace=True)\n",
    "    f_noun_answer.replace('performance%1:22:00::','q',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('plan%1:09:00::','r',inplace=True)\n",
    "    f_noun_answer.replace('plan%1:09:01::','r',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('shelter%1:06:00::','s',inplace=True)\n",
    "    f_noun_answer.replace('shelter%1:06:01::','s',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('shelter%1:21:00::','t',inplace=True)\n",
    "    f_noun_answer.replace('shelter%1:26:00::','t',inplace=True)\n",
    "\n",
    "    f_noun_answer.replace('source%1:10:00::','u',inplace=True)\n",
    "    f_noun_answer.replace('source%1:10:01::','u',inplace=True)\n",
    "    f_noun_answer.replace('source%1:18:00::','u',inplace=True)\n",
    "\n",
    "\n",
    "    #\n",
    "    #answer\n",
    "    #=-----------------\n",
    "    f_noun_true.replace('argument%1:10:00::','a',inplace=True)\n",
    "    f_noun_true.replace('argument%1:10:00::','a',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('bank%1:06:00::','b',inplace=True)\n",
    "    f_noun_true.replace('bank%1:06:01::','b',inplace=True)\n",
    "    f_noun_true.replace('bank%1:14:00::','b',inplace=True)\n",
    "    f_noun_true.replace('bank%1:14:01::','b',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('bank%1:17:00::','c',inplace=True)\n",
    "    f_noun_true.replace('bank%1:17:01::','c',inplace=True)\n",
    "    f_noun_true.replace('bank%1:17:02::','c',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('bank%1:21:00::','d',inplace=True)\n",
    "    f_noun_true.replace('bank%1:21:01::','d',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('degree%1:07:00::','e',inplace=True)\n",
    "    f_noun_true.replace('degree%1:07:01::','e',inplace=True)\n",
    "    f_noun_true.replace('degree%1:26:01::','e',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('image%1:06:00::','f',inplace=True)\n",
    "    f_noun_true.replace('image%1:06:01::','f',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('image%1:07:00::','g',inplace=True)\n",
    "    f_noun_true.replace('image%1:09:00::','g',inplace=True)\n",
    "    f_noun_true.replace('image%1:09:02::','g',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('interest%1:04:01::','h',inplace=True)\n",
    "    f_noun_true.replace('interest%1:07:01::','h',inplace=True)\n",
    "    f_noun_true.replace('interest%1:07:02::','h',inplace=True)\n",
    "    f_noun_true.replace('interest%1:09:00::','h',inplace=True)\n",
    "    f_noun_true.replace('interest%1:14:00::','h',inplace=True)\n",
    "    f_noun_true.replace('interest%1:21:00::','i',inplace=True)\n",
    "    f_noun_true.replace('interest%1:21:03::','i',inplace=True)\n",
    "\n",
    "    #------------------\n",
    "    f_noun_true.replace('judgment%1:04:00::','j',inplace=True)\n",
    "    f_noun_true.replace('judgment%1:04:02::','j',inplace=True)\n",
    "    f_noun_true.replace('judgment%1:10:00::','j',inplace=True)\n",
    "    f_noun_true.replace('judgment%1:07:00::','k',inplace=True)\n",
    "    f_noun_true.replace('judgment%1:09:00::','k',inplace=True)\n",
    "    f_noun_true.replace('judgment%1:09:01::','k',inplace=True)\n",
    "    f_noun_true.replace('judgment%1:09:04::','k',inplace=True)\n",
    "    #----------------\n",
    "\n",
    "    f_noun_true.replace('paper%1:10:01::','l',inplace=True)\n",
    "    f_noun_true.replace('paper%1:10:02::','l',inplace=True)\n",
    "    f_noun_true.replace('paper%1:06:00::','m',inplace=True)\n",
    "    f_noun_true.replace('paper%1:10:03::','m',inplace=True)\n",
    "    f_noun_true.replace('paper%1:14:00::','m',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('paper%1:10:00::','n',inplace=True)\n",
    "    f_noun_true.replace('paper%1:27:00::','n',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('party%1:11:00::','o',inplace=True)\n",
    "    f_noun_true.replace('party%1:14:02::','o',inplace=True)\n",
    "    f_noun_true.replace('party%1:18:00::','o',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('performance%1:04:00::','p',inplace=True)\n",
    "    f_noun_true.replace('performance%1:04:01::','p',inplace=True)\n",
    "    f_noun_true.replace('performance%1:10:00::','p',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('performance%1:04:03::','q',inplace=True)\n",
    "    f_noun_true.replace('performance%1:22:00::','q',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('plan%1:09:00::','r',inplace=True)\n",
    "    f_noun_true.replace('plan%1:09:01::','r',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('shelter%1:06:00::','s',inplace=True)\n",
    "    f_noun_true.replace('shelter%1:06:01::','s',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('shelter%1:21:00::','t',inplace=True)\n",
    "    f_noun_true.replace('shelter%1:26:00::','t',inplace=True)\n",
    "\n",
    "    f_noun_true.replace('source%1:10:00::','u',inplace=True)\n",
    "    f_noun_true.replace('source%1:10:01::','u',inplace=True)\n",
    "    f_noun_true.replace('source%1:18:00::','u',inplace=True)\n",
    "\n",
    "    #Verb  合并后意思 4.07， 27个单词， 1698 个例子\n",
    "\n",
    "    f_answer.replace('42602','a',inplace=True)\n",
    "    f_answer.replace('42605','a',inplace=True)\n",
    "    f_answer.replace('238101','b',inplace=True)\n",
    "    f_answer.replace('238105','b',inplace=True)\n",
    "    f_answer.replace('238102','c',inplace=True)\n",
    "    f_answer.replace('238104','c',inplace=True)\n",
    "    f_answer.replace('238106','c',inplace=True)  # delete 'begin'\n",
    "    f_answer.replace('770001','d',inplace=True)\n",
    "    f_answer.replace('770002','d',inplace=True)   #delete 'decide'\n",
    "    f_answer.replace('770005','d',inplace=True)\n",
    "    f_answer.replace('1297001','e',inplace=True)\n",
    "    f_answer.replace('1297006','e',inplace=True)\n",
    "    f_answer.replace('1297002','f',inplace=True)\n",
    "    f_answer.replace('1297007','f',inplace=True)\n",
    "    f_answer.replace('1353101','g',inplace=True)  #delete expect,express\n",
    "    f_answer.replace('1353103','g',inplace=True)\n",
    "    f_answer.replace('1892101','h',inplace=True)\n",
    "    f_answer.replace('1892103','h',inplace=True)\n",
    "    f_answer.replace('1892105','h',inplace=True)\n",
    "\n",
    "    f_answer.replace('1892102','i',inplace=True)\n",
    "    f_answer.replace('1892104','i',inplace=True)\n",
    "    f_answer.replace('1892106','i',inplace=True)\n",
    "    f_answer.replace('1892107','i',inplace=True)\n",
    "\n",
    "    f_answer.replace('2439901','j',inplace=True)\n",
    "    f_answer.replace('2439902','j',inplace=True)\n",
    "    f_answer.replace('2439904','j',inplace=True)\n",
    "    f_answer.replace('2439905','j',inplace=True)\n",
    "\n",
    "    f_answer.replace('2439903','k',inplace=True)\n",
    "    f_answer.replace('2439908','k',inplace=True)\n",
    "    f_answer.replace('2439906','k',inplace=True)\n",
    "\n",
    "    f_answer.replace('2555501','l',inplace=True)\n",
    "    f_answer.replace('2555502','l',inplace=True)\n",
    "    f_answer.replace('2555507','l',inplace=True)\n",
    "\n",
    "    f_answer.replace('2555504','m',inplace=True)\n",
    "    f_answer.replace('2555505','m',inplace=True)\n",
    "    f_answer.replace('2555506','m',inplace=True)\n",
    "\n",
    "    f_answer.replace('2644301','n',inplace=True)\n",
    "    f_answer.replace('2644307','n',inplace=True)\n",
    "    f_answer.replace('2644302','n',inplace=True)\n",
    "    f_answer.replace('2644303','n',inplace=True)   # delete note\n",
    "\n",
    "    f_answer.replace('3165218','o',inplace=True)\n",
    "    f_answer.replace('3165217','o',inplace=True)\n",
    "    f_answer.replace('3165213','o',inplace=True)\n",
    "\n",
    "    f_answer.replace('3165220','p',inplace=True)\n",
    "    f_answer.replace('3165214','p',inplace=True)\n",
    "\n",
    "    f_answer.replace('3288301','q',inplace=True)\n",
    "    f_answer.replace('3288306','q',inplace=True)\n",
    "\n",
    "    f_answer.replace('3288302','r',inplace=True)\n",
    "    f_answer.replace('3288305','r',inplace=True)\n",
    "\n",
    "    f_answer.replace('3313901','s',inplace=True)\n",
    "    f_answer.replace('3313906','s',inplace=True)\n",
    "    f_answer.replace('3313902','t',inplace=True)\n",
    "    f_answer.replace('3313905','t',inplace=True)\n",
    "\n",
    "    f_answer.replace('3434801','u',inplace=True)\n",
    "    f_answer.replace('3434806','u',inplace=True)\n",
    "\n",
    "    f_answer.replace('3434803','v',inplace=True)\n",
    "    f_answer.replace('3434807','v',inplace=True)\n",
    "\n",
    "    f_answer.replace('3477801','w',inplace=True)\n",
    "    f_answer.replace('3477803','w',inplace=True)\n",
    "\n",
    "    f_answer.replace('3597906','x',inplace=True)\n",
    "    f_answer.replace('3597907','x',inplace=True)\n",
    "    f_answer.replace('3597911','x',inplace=True)\n",
    "\n",
    "    f_answer.replace('3893501','y',inplace=True)\n",
    "    f_answer.replace('3893505','y',inplace=True)\n",
    "    f_answer.replace('3893507','y',inplace=True)\n",
    "    f_answer.replace('3893508','y',inplace=True)\n",
    "\n",
    "    f_answer.replace('3893502','z',inplace=True)\n",
    "    f_answer.replace('3893503','z',inplace=True)\n",
    "    f_answer.replace('3893509','z',inplace=True)\n",
    "\n",
    "    f_answer.replace('4155302','aa',inplace=True)\n",
    "    f_answer.replace('4155303','aa',inplace=True)\n",
    "    f_answer.replace('4155304','aa',inplace=True)\n",
    "    f_answer.replace('4155307','aa',inplace=True)\n",
    "\n",
    "    f_answer.replace('4198501','bb',inplace=True)\n",
    "    f_answer.replace('4198502','bb',inplace=True)\n",
    "    f_answer.replace('4198503','bb',inplace=True)\n",
    "    f_answer.replace('4198506','bb',inplace=True)\n",
    "    f_answer.replace('4198507','bb',inplace=True)  \n",
    "\n",
    "    f_answer.replace('4380101','cc',inplace=True)\n",
    "    f_answer.replace('4380102','cc',inplace=True)\n",
    "    f_answer.replace('4380103','cc',inplace=True)\n",
    "\n",
    "    f_answer.replace('4380104','dd',inplace=True)\n",
    "    f_answer.replace('4380105','dd',inplace=True)\n",
    "\n",
    "    #--------------------------------------------\n",
    "    f_answer.replace('4380106','ee',inplace=True)\n",
    "    f_answer.replace('4380109','ee',inplace=True)\n",
    "\n",
    "    f_answer.replace('4530702','ff',inplace=True)\n",
    "    f_answer.replace('4530704','ff',inplace=True)\n",
    "\n",
    "    f_answer.replace('4636102','gg',inplace=True) \n",
    "    f_answer.replace('4636101','gg',inplace=True) \n",
    "    f_answer.replace('4636107','gg',inplace=True) \n",
    "    f_answer.replace('4636108','gg',inplace=True) \n",
    "\n",
    "    f_answer.replace('4636103','hh',inplace=True) \n",
    "    f_answer.replace('4636104','hh',inplace=True) \n",
    "    f_answer.replace('4636110','hh',inplace=True) \n",
    "    f_answer.replace('4636111','hh',inplace=True) \n",
    "    f_answer.replace('4636112','hh',inplace=True) \n",
    "    #---------------\n",
    "\n",
    "\n",
    "    f_answer.replace('4640501','ii',inplace=True) \n",
    "    f_answer.replace('4640502','ii',inplace=True) \n",
    "    f_answer.replace('4640507','ii',inplace=True)\n",
    "\n",
    "    f_answer.replace('4640503','jj',inplace=True) \n",
    "    f_answer.replace('4640508','jj',inplace=True) \n",
    "    f_answer.replace('4640509','jj',inplace=True) \n",
    "    #------------------\n",
    "    f_answer.replace('4711401','kk',inplace=True) \n",
    "    f_answer.replace('4711403','kk',inplace=True) \n",
    "    f_answer.replace('4711405','kk',inplace=True)\n",
    "\n",
    "    f_answer.replace('4711404','ll',inplace=True) \n",
    "    f_answer.replace('4711406','ll',inplace=True) \n",
    "\n",
    "    #---------------------------------\n",
    "    f_answer.replace('4753401','mm',inplace=True) \n",
    "    f_answer.replace('4753404','mm',inplace=True) \n",
    "    f_answer.replace('4753406','mm',inplace=True)\n",
    "\n",
    "    f_answer.replace('4753403','nn',inplace=True) \n",
    "    f_answer.replace('4753407','nn',inplace=True)\n",
    "\n",
    "    f_answer.replace('4753405','oo',inplace=True) \n",
    "    f_answer.replace('4753408','oo',inplace=True)\n",
    "\n",
    "    #\n",
    "    # 答案\n",
    "    #---------------------------------------------------------\n",
    "    f_true.replace('42602','a',inplace=True)\n",
    "    f_true.replace('42605','a',inplace=True)\n",
    "    f_true.replace('238101','b',inplace=True)\n",
    "    f_true.replace('238105','b',inplace=True)\n",
    "    f_true.replace('238102','c',inplace=True)\n",
    "    f_true.replace('238104','c',inplace=True)\n",
    "    f_true.replace('238106','c',inplace=True)  # delete 'begin'\n",
    "    f_true.replace('770001','d',inplace=True)\n",
    "    f_true.replace('770002','d',inplace=True)   #delete 'decide'\n",
    "    f_true.replace('770005','d',inplace=True)\n",
    "    f_true.replace('1297001','e',inplace=True)\n",
    "    f_true.replace('1297006','e',inplace=True)\n",
    "    f_true.replace('1297002','f',inplace=True)\n",
    "    f_true.replace('1297007','f',inplace=True)\n",
    "    f_true.replace('1353101','g',inplace=True)  #delete expect,express\n",
    "    f_true.replace('1353103','g',inplace=True)\n",
    "    f_true.replace('1892101','h',inplace=True)\n",
    "    f_true.replace('1892103','h',inplace=True)\n",
    "    f_true.replace('1892105','h',inplace=True)\n",
    "    f_true.replace('1892102','i',inplace=True)\n",
    "    f_true.replace('1892104','i',inplace=True)\n",
    "    f_true.replace('1892106','i',inplace=True)\n",
    "    f_true.replace('1892107','i',inplace=True)\n",
    "    f_true.replace('2439901','j',inplace=True)\n",
    "    f_true.replace('2439902','j',inplace=True)\n",
    "    f_true.replace('2439904','j',inplace=True)\n",
    "    f_true.replace('2439905','j',inplace=True)\n",
    "    f_true.replace('2439903','k',inplace=True)\n",
    "    f_true.replace('2439908','k',inplace=True)\n",
    "    f_true.replace('2439906','k',inplace=True)\n",
    "    f_true.replace('2555501','l',inplace=True)\n",
    "    f_true.replace('2555502','l',inplace=True)\n",
    "    f_true.replace('2555507','l',inplace=True)\n",
    "    f_true.replace('2555504','m',inplace=True)\n",
    "    f_true.replace('2555505','m',inplace=True)\n",
    "    f_true.replace('2555506','m',inplace=True)\n",
    "    f_true.replace('2644301','n',inplace=True)\n",
    "    f_true.replace('2644307','n',inplace=True)\n",
    "    f_true.replace('2644302','n',inplace=True)\n",
    "    f_true.replace('2644303','n',inplace=True)   # delete note\n",
    "    f_true.replace('3165218','o',inplace=True)\n",
    "    f_true.replace('3165217','o',inplace=True)\n",
    "    f_true.replace('3165213','o',inplace=True)\n",
    "    f_true.replace('3165220','p',inplace=True)\n",
    "    f_true.replace('3165214','p',inplace=True)\n",
    "    f_true.replace('3288301','q',inplace=True)\n",
    "    f_true.replace('3288306','q',inplace=True)\n",
    "    f_true.replace('3288302','r',inplace=True)\n",
    "    f_true.replace('3288305','r',inplace=True)\n",
    "    f_true.replace('3313901','s',inplace=True)\n",
    "    f_true.replace('3313906','s',inplace=True)\n",
    "    f_true.replace('3313902','t',inplace=True)\n",
    "    f_true.replace('3313905','t',inplace=True)\n",
    "    f_true.replace('3434801','u',inplace=True)\n",
    "    f_true.replace('3434806','u',inplace=True)\n",
    "    f_true.replace('3434803','v',inplace=True)\n",
    "    f_true.replace('3434807','v',inplace=True)\n",
    "    f_true.replace('3477801','w',inplace=True)\n",
    "    f_true.replace('3477803','w',inplace=True)\n",
    "    f_true.replace('3597906','x',inplace=True)\n",
    "    f_true.replace('3597907','x',inplace=True)\n",
    "    f_true.replace('3597911','x',inplace=True)\n",
    "    f_true.replace('3893501','y',inplace=True)\n",
    "    f_true.replace('3893505','y',inplace=True)\n",
    "    f_true.replace('3893507','y',inplace=True)\n",
    "    f_true.replace('3893508','y',inplace=True)\n",
    "    f_true.replace('3893502','z',inplace=True)\n",
    "    f_true.replace('3893503','z',inplace=True)\n",
    "    f_true.replace('3893509','z',inplace=True)\n",
    "    f_true.replace('4155302','aa',inplace=True)\n",
    "    f_true.replace('4155303','aa',inplace=True)\n",
    "    f_true.replace('4155304','aa',inplace=True)\n",
    "    f_true.replace('4155307','aa',inplace=True)\n",
    "    f_true.replace('4198501','bb',inplace=True)\n",
    "    f_true.replace('4198502','bb',inplace=True)\n",
    "    f_true.replace('4198503','bb',inplace=True)\n",
    "    f_true.replace('4198506','bb',inplace=True)\n",
    "    f_true.replace('4198507','bb',inplace=True)  \n",
    "    f_true.replace('4380101','cc',inplace=True)\n",
    "    f_true.replace('4380102','cc',inplace=True)\n",
    "    f_true.replace('4380103','cc',inplace=True)\n",
    "    f_true.replace('4380104','dd',inplace=True)\n",
    "    f_true.replace('4380105','dd',inplace=True)\n",
    "\n",
    "    #--------------------------------------------\n",
    "    f_true.replace('4380106','ee',inplace=True)\n",
    "    f_true.replace('4380109','ee',inplace=True)\n",
    "    f_true.replace('4530702','ff',inplace=True)\n",
    "    f_true.replace('4530704','ff',inplace=True)\n",
    "\n",
    "    f_true.replace('4636102','gg',inplace=True) \n",
    "    f_true.replace('4636101','gg',inplace=True) \n",
    "    f_true.replace('4636107','gg',inplace=True) \n",
    "    f_true.replace('4636108','gg',inplace=True) \n",
    "    f_true.replace('4636103','hh',inplace=True) \n",
    "    f_true.replace('4636104','hh',inplace=True) \n",
    "    f_true.replace('4636110','hh',inplace=True) \n",
    "    f_true.replace('4636111','hh',inplace=True) \n",
    "    f_true.replace('4636112','hh',inplace=True) \n",
    "    #---------------\n",
    "\n",
    "    f_true.replace('4640501','ii',inplace=True) \n",
    "    f_true.replace('4640502','ii',inplace=True) \n",
    "    f_true.replace('4640507','ii',inplace=True)\n",
    "    f_true.replace('4640503','jj',inplace=True) \n",
    "    f_true.replace('4640508','jj',inplace=True) \n",
    "    f_true.replace('4640509','jj',inplace=True) \n",
    "    #------------------\n",
    "    f_true.replace('4711401','kk',inplace=True) \n",
    "    f_true.replace('4711403','kk',inplace=True) \n",
    "    f_true.replace('4711405','kk',inplace=True)\n",
    "\n",
    "    f_true.replace('4711404','ll',inplace=True) \n",
    "    f_true.replace('4711406','ll',inplace=True) \n",
    "\n",
    "    #---------------------------------\n",
    "    f_true.replace('4753401','mm',inplace=True) \n",
    "    f_true.replace('4753404','mm',inplace=True) \n",
    "    f_true.replace('4753406','mm',inplace=True)\n",
    "    f_true.replace('4753403','nn',inplace=True) \n",
    "    f_true.replace('4753407','nn',inplace=True)\n",
    "    f_true.replace('4753405','oo',inplace=True) \n",
    "    f_true.replace('4753408','oo',inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    #ADJ  合并后意思 2.75， 141个例子， 4个单词\n",
    "\n",
    "    f_adj_answer.replace('hot%3:00:01::','a',inplace=True)\n",
    "    f_adj_answer.replace('hot%3:00:02::','a',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:active:01','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:charged:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:fast:01','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:fresh:01','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:good:01','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:illegal:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:lucky:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:near:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:new:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:popular:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:pungent:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:radioactive:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:sexy:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:skilled:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:unpleasant:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:violent:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:wanted:00','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:00:warm:03','b',inplace=True)\n",
    "    f_adj_answer.replace('hot%5:00:02:fast:01','b',inplace=True)\n",
    "    f_adj_answer.replace('important%3:00:00::','c',inplace=True)\n",
    "    f_adj_answer.replace('important%3:00:02::','c',inplace=True)\n",
    "    f_adj_answer.replace('important%3:00:04::','c',inplace=True)\n",
    "    f_adj_answer.replace('important%5:00:00:immodest:02','d',inplace=True)\n",
    "    f_adj_answer.replace('important%5:00:00:influential:00','d',inplace=True)\n",
    "    f_adj_answer.replace('solid%3:00:01::','e',inplace=True)\n",
    "    f_adj_answer.replace('solid%3:00:02::','e',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:homogeneous:00','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:honorable:00','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:opaque:00','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:plain:02','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:sound:01','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:unbroken:02','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:undiversified:00','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:wholesome:00','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:hard:01','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:good:01','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:frozen:00','f',inplace=True)\n",
    "    f_adj_answer.replace('solid%5:00:00:cubic:00','f',inplace=True)\n",
    "\n",
    "    #\n",
    "    #answer\n",
    "    #-----------------\n",
    "\n",
    "    f_adj_true.replace('hot%3:00:01::','a',inplace=True)\n",
    "    f_adj_true.replace('hot%3:00:02::','a',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:active:01','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:charged:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:fast:01','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:fresh:01','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:good:01','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:illegal:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:lucky:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:near:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:new:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:popular:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:pungent:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:radioactive:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:sexy:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:skilled:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:unpleasant:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:violent:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:wanted:00','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:00:warm:03','b',inplace=True)\n",
    "    f_adj_true.replace('hot%5:00:02:fast:01','b',inplace=True)\n",
    "    f_adj_true.replace('important%3:00:00::','c',inplace=True)\n",
    "    f_adj_true.replace('important%3:00:02::','c',inplace=True)\n",
    "    f_adj_true.replace('important%3:00:04::','c',inplace=True)\n",
    "    f_adj_true.replace('important%5:00:00:immodest:02','d',inplace=True)\n",
    "    f_adj_true.replace('important%5:00:00:influential:00','d',inplace=True)\n",
    "    f_adj_true.replace('solid%3:00:01::','e',inplace=True)\n",
    "    f_adj_true.replace('solid%3:00:02::','e',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:homogeneous:00','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:honorable:00','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:opaque:00','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:plain:02','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:sound:01','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:unbroken:02','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:undiversified:00','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:wholesome:00','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:hard:01','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:good:01','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:frozen:00','f',inplace=True)\n",
    "    f_adj_true.replace('solid%5:00:00:cubic:00','f',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
